{"meta":{"title":"泡码侠","subtitle":"如泡茶一样，品味代码","description":"个人博客","author":"TenYoDun","url":"https://blog.paomax.com","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2021-02-20T05:08:10.485Z","updated":"2021-02-20T05:08:10.485Z","comments":false,"path":"/404.html","permalink":"https://blog.paomax.com/404.html","excerpt":"","text":""},{"title":"关于","date":"2021-02-20T05:08:10.511Z","updated":"2021-02-20T05:08:10.511Z","comments":false,"path":"about/index.html","permalink":"https://blog.paomax.com/about/index.html","excerpt":"","text":"热爱新鲜事物，有一颗永远充满极客的热情的心，关注计算机技术，对编程痴狂 个人信息手册 项目 值 name 泡码侠 sex 男 birthday 1998-03-03 address 山东潍坊 education 暂为本科 github https://github.com/TenYoDun blog https://blog.paomax.com/ email 291746925@qq.com description 热爱一切新鲜事物 skills git,github,C++,C,Python,java,php,HTML,CSS,JavaScript,机器学习,神经网络,深度学习,前端,后端,计算机 devTools VSCode,git,PyCharm,PhpStorm,WinSCP,WindowsTerminal,Eclipse,CodeBlocks"},{"title":"分类","date":"2021-02-20T05:08:10.511Z","updated":"2021-02-20T05:08:10.511Z","comments":false,"path":"categories/index.html","permalink":"https://blog.paomax.com/categories/index.html","excerpt":"","text":""},{"title":"img","date":"2020-03-04T10:33:42.000Z","updated":"2021-02-20T05:08:10.522Z","comments":true,"path":"img/index.html","permalink":"https://blog.paomax.com/img/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2021-02-20T05:08:10.523Z","updated":"2021-02-20T05:08:10.523Z","comments":true,"path":"links/index.html","permalink":"https://blog.paomax.com/links/index.html","excerpt":"","text":""},{"title":"项目","date":"2021-02-20T05:08:10.524Z","updated":"2021-02-20T05:08:10.524Z","comments":false,"path":"repository/index.html","permalink":"https://blog.paomax.com/repository/index.html","excerpt":"","text":""},{"title":"标签","date":"2021-02-20T05:08:10.525Z","updated":"2021-02-20T05:08:10.525Z","comments":false,"path":"tags/index.html","permalink":"https://blog.paomax.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"ProUM算法-V2","slug":"ProUM算法-V2","date":"2020-05-19T11:00:28.000Z","updated":"2021-03-03T01:33:56.797Z","comments":true,"path":"2020/05/19/1900.html","link":"","permalink":"https://blog.paomax.com/2020/05/19/1900.html","excerpt":"第二个版本，代码优化了很多，基本符合论文思路，写这一遍的最大感触就是python的字典机制，注意浅复制和深复制","text":"第二个版本，代码优化了很多，基本符合论文思路，写这一遍的最大感触就是python的字典机制，注意浅复制和深复制 ProUm算法 database.txt-数据库文件 12345a[6] c[10] -1 c[20] -1 b[20] f[3] -1 a[6] e[6] -1 -2 SUtility:71f[2] -1 a[15] d[8] -1 c[20] -1 b[8] -1 a[12] d[4] -1 -2 SUtility:69a[12] -1 b[8] -1 f[5] -1 a[3] b[4] e[6] -1 -2 SUtility:38a[9] b[8] d[20] -1 c[20] e[6] -1 -2 SUtility:63b[2] e[6] -1 c[10] -1 f[2] -1 d[8] -1 a[12] e[12] -1 -2 SUtility:52 工具类函数 utils.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121def rest(S, s): \"\"\" 计算剩余效应值 :param S:单行数据 :param s:需要计算剩余效用值的一项集 :return: rest:剩余效应值 \"\"\" rest = 0 # 剩余效应值 s = s[-1] # 取最后一项 key_1 = list(s.values())[0] # 找到键值1 # 找u(&lt;S-t&gt;)rest,行S中的剩余效应值 # 先找第一个匹配项 if ('[' in list(s.keys())[0]): # 带‘[’说明键值大于两项，如&lt;(a,b)&gt; i = eval(list(s.keys())[0])[-1] # 找到如&lt;(a,b)&gt;的最后一个b的下标 keys = list(S[key_1].keys()) m = keys[keys.index(i) + 1:] for key_2 in m: rest += S[key_1][key_2] # 计算当前项的效用值 else: keys = list(S[key_1].keys()) i = list(s.keys())[0] m = keys[keys.index(i) + 1:] for key_2 in m: rest += S[key_1][key_2] # 计算当前项的效用值 for s in S[key_1 + 1:len(S) - 1]: # 去掉最后一个项的行总效应值项 rest += sum(s.values()) return restdef make_utility_array(S): \"\"\" 生成一行数据的投影数组 :param S:一行数据 :return:utility_array:一行数据的投影效应数组 \"\"\" i = 0 # 计数 utility_array = [] # 临时array for s in S: # 遍历数据库行 eid = S.index(s) # eid for item, u in s.items(): # 遍历每一行的每一项 ru = rest(S, [&#123;item: eid&#125;]) utility_array.append(&#123;'position': i, 'eid': eid, 'item': item, 'u': u, 'ru': ru&#125;) i += 1 # position utility_array = utility_array[:-1] # 去掉最后一项无用项 for i in range(len(utility_array)): # 遍历查找next_position,next_eid for j in range(i + 1, len(utility_array)): # 找next_position if utility_array[i]['item'] == utility_array[j]['item']: # 如果元素相同，说明找到 utility_array[i].update(&#123;'next_pos': utility_array[j]['position']&#125;) # 则将找到元素的index赋值给next_position break elif j == len(utility_array) - 1: # 找到最后没找到 utility_array[i].update(&#123;'next_pos': '-'&#125;) if i == len(utility_array) - 1: # 最后一个元素 utility_array[i].update(&#123;'next_pos': '-'&#125;) for j in range(i + 1, len(utility_array)): # 找next_eid if utility_array[i]['eid'] != utility_array[j]['eid']: # 如果eid不同，说明找到 utility_array[i].update(&#123;'next_eid': utility_array[j]['position']&#125;) # 则将找到元素的index赋值给next_position break elif j == len(utility_array) - 1: # 找到最后没找到 utility_array[i].update(&#123;'next_eid': '-'&#125;) if i == len(utility_array) - 1: # 最后一个元素 utility_array[i].update(&#123;'next_eid': '-'&#125;) return utility_arraydef find_max_value(item, items): \"\"\" :param item:要查找的项 :param items:查找范围 找当前项在投影数据库中的最大值 :return: max_value:最大值 \"\"\" max_value = item['u'] for i in range(len(items)): if item['next_pos'] != '-': next_pos = item['next_pos'] for temp in items: if temp['position'] == next_pos: next_pos = items.index(temp) temp_value = items[next_pos]['u'] if temp_value &gt; max_value: max_value = temp_value item = items[next_pos] # item变成新的位置 else: break return max_valuedef delete_item(one_item, D): \"\"\" 从D中删除小于最小效应值的一频繁项集 :param one_item:待删除的一频繁项集 :param D:格式化的数据库 :return: \"\"\" new_D = [] for S in D: new_S = [] for t in S[:-1]: if set(one_item) &lt;= set(t.keys()): t.pop(one_item) if len(t)==0: continue new_S.append(t) new_S.append(S[-1]) new_D.append(new_S) return new_Ddef set(list): \"\"\" 去重函数 :param list:待去重列表 :return: new_list:去重后的列表 \"\"\" new_list = [] for id in list: if id not in new_list: new_list.append(id) return new_list 算法总体函数 proUM.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262import utilsimport copydef data_set(filename): \"\"\"解析数据库，转换成容易处理的数据 :param filename:数据库文件 :return: D:格式化后的数据 \"\"\" D = [] # 定义空列表，准备存储数据 with open(filename, 'r') as input: # 读取文件 for line in input.readlines(): # 一次读取一行 elements = line.replace('[', '=').replace(']', '|').replace(' ', '').split('-1') # 格式化数据 line = [] # 定义空行 for e_1 in elements[0:-1]: # 遍历一行序列，除去最后一个行的总效应值元素 e_1 = e_1.split('|')[0:-1] # 这里分割最后面一个元素会出现一个空格，所以去掉最后一个空格不要 item = &#123;&#125; # 定义空项 for e_2 in e_1: # 遍历其中一项 e_2 = e_2.split('=') # 分隔项名称和效应值 t = &#123;e_2[0]: int(e_2[1])&#125; # 第二个是=标识符，所以从选择第1个和3个元素，下表是0和2 item.update(t) line.append(item) utility = &#123;'SUtility': int(elements[-1][11:])&#125; # 取出最后一项，得到行效用值 line.append(utility) D.append(line) return D # 返回最终处理后的序列数据库def construct_utility_array(D): \"\"\" 构建数据库总效应数组 :param D:格式化后的数据 :return: utility_array:效应数组 \"\"\" D_utility_array = [] for S in D: utility_array = utils.make_utility_array(S) # 调用生成行效应数组 D_utility_array.append(utility_array) return D_utility_arraydef construct_projected_utility_array(pre, pre_utility_array): \"\"\" 构建投影效应数组 :param pre: 前缀，格式：&#123;'item': ['a', 'b'], 'last_eid': [1,2,3,2,1], 'sum_value': [30,50,32,5,1]&#125; :param pre_utility_array: 上一级（父级）的投影效应数组，这里暂且叫前效应数组 :return: projected_utility_array:投影数组 \"\"\" projected_utility_array = [] # 定义空投影数组 for arrays in pre_utility_array: # 遍历前效应数组 index = pre_utility_array.index(arrays) # 找前缀对应的index值 start_eid = pre['last_eid'][index] # 起始位置eid last_item = pre['item'][-1][-1] start_pos = -1 # 起始位置：position for item in arrays: if item['eid'] == start_eid and item['item'] == last_item: start_pos = arrays.index(item) break if start_pos == -1 and len(pre['item']) != 0: # 等于-1说明没找到，直接给空数组 utility_array = [] else: utility_array = arrays[start_pos + 1:] # 某一行的投影数组 projected_utility_array.append(utility_array) # 加入到总投影数组中 return projected_utility_arraydef i_concatenation(pre, projected_utility_array): \"\"\"项扩展 :param pre:前缀 :param projected_utility_array:投影数据库 :return:项扩展字典 \"\"\" last_eid = [-1 for i in range(len(projected_utility_array))] # 用于存放last_eid u = [0 for i in range(len(projected_utility_array))] # 用于存放扩展项的效应值 seu = [0 for i in range(len(projected_utility_array))] # 用于存放每一行的seu i_items_dic = &#123;&#125; # 存储扩展序列，格式为：&#123;a:&#123;last_eid:[],seu:&#125;&#125; for line in projected_utility_array: index = projected_utility_array.index(line) for item in line: if pre['last_eid'][index] == item['eid'] or len(pre['item']) == 0: # 如果前缀最后一项和当前item所在eid相同，或者为空,则扩展 if set(item['item']) &lt; set(i_items_dic.keys()): # 如果此前缀已经存在items中 i_items_dic[item['item']]['seu'][index] = item['seu'] # 行seu值 if i_items_dic[item['item']]['last_eid'][index] == -1: # 只赋值第一个匹配项的eid i_items_dic[item['item']]['last_eid'][index] = item['eid'] # 赋值eid i_items_dic[item['item']]['u'][index] = item['u'] # 赋值u else: # 若不存在 i_items_dic.update( &#123;item['item']: &#123;'last_eid': last_eid.copy(), 'u': u.copy(), 'seu': seu.copy()&#125;&#125;) # 新加入seu i_items_dic[item['item']]['last_eid'][index] = item['eid'] # 赋值eid i_items_dic[item['item']]['u'][index] = item['u'] # 赋值u i_items_dic[item['item']]['seu'][index] = item['seu'] return i_items_dicdef s_concatenation(pre, projected_utility_array): \"\"\"序列扩展 :param pre:前缀 :param projected_utility_array:投影数据库 :return:序列扩展字典 \"\"\" last_eid = [-1 for i in range(len(projected_utility_array))] # 用于存放last_eid u = [0 for i in range(len(projected_utility_array))] # 用于存放扩展项的效应值 seu = [0 for i in range(len(projected_utility_array))] # 用于存放每一行的seu s_items_dic = &#123;&#125; # 存储扩展序列 for line in projected_utility_array: index = projected_utility_array.index(line) for item in line: if pre['last_eid'][index] != item['eid'] and pre['last_eid'][index] != -1 or len( pre['item']) == 0: # 如果前缀最后一项和当前item所在eid相同，或者为空，则扩展 if set(item['item']) &lt; set(s_items_dic.keys()): # 如果此前缀已经存在items中 s_items_dic[item['item']]['seu'][index] = item['seu'] # 行seu值 if s_items_dic[item['item']]['last_eid'][index] == -1: # 只赋值第一个匹配项的eid s_items_dic[item['item']]['last_eid'][index] = item['eid'] # 赋值eid s_items_dic[item['item']]['u'][index] = item['u'] # 赋值u else: # 若不存在 s_items_dic.update( &#123;item['item']: &#123;'last_eid': last_eid.copy(), 'u': u.copy(), 'seu': seu.copy()&#125;&#125;) # 则加入 s_items_dic[item['item']]['last_eid'][index] = item['eid'] # 赋值eid s_items_dic[item['item']]['u'][index] = item['u'] # 赋值u s_items_dic[item['item']]['seu'][index] = item['seu'] return s_items_dicdef i_connect(pre, i): \"\"\" 把生成的项扩展序列和前缀进行连接 :param pre: 前缀 :param i: 项扩展序列 :return: 组合后的新前缀 \"\"\" new_pre = copy.deepcopy(pre) item = list(i.keys())[0] if len(new_pre['item']) == 0: new_pre['item'].append([item]) else: new_pre['item'][-1].append(item) # 则直接加入 new_pre['last_eid'] = i[item]['last_eid'] # 赋值last_eid for x in range(len(new_pre['sum_value'])): # 计算此项的效应和 new_pre['sum_value'][x] = new_pre['sum_value'][x] + i[item]['u'][x] return new_predef s_connect(pre, i): \"\"\" 把生成的序列扩展序列和前缀进行连接 :param pre: 前缀 :param i: 项扩展序列 :return: 组合后的新前缀 \"\"\" new_pre = copy.deepcopy(pre) item = list(i.keys())[0] new_pre['item'].append([item]) new_pre['last_eid'] = i[item]['last_eid'] # 赋值last_eid for x in range(len(new_pre['sum_value'])): # 计算此项的效应和 new_pre['sum_value'][x] = new_pre['sum_value'][x] + i[item]['u'][x] return new_predef SWU(one_sequences, D): \"\"\"计算SWU值 :param one_sequences:需要计算SWU的一频繁项集 :param D:格式化后的数据库 :return: swu: swu值 \"\"\" swu = 0 # 记录swu值 for S in D: # 遍历数据库 for item in S: # 遍历一行中的元素 item_key = set(item.keys()) # 转换成结合 if set(one_sequences) &lt;= item_key: # 如果在这一行中 swu += S[-1]['SUtility'] break return swudef SEU(projected_utility_array): \"\"\" 生成附带SEU值的投影数组，方便以后计算扩展序列的SEU，并且减少系统性能开销 这里计算的是一项集在一行数据S中的SEU :param projected_utility_array:投影数组 :return: projected_utility_array:计算好SEU值的投影数组 \"\"\" for items in projected_utility_array: # 遍历投影数组一行 seu_dic = &#123;&#125; # 用于记录当前items的所有seu for item in items: # 遍历每一个效应数组 if set(item['item']) &lt;= set(seu_dic.keys()): # 如果当前元素已经计算出了seu item['seu'] = seu_dic[item['item']] # 就把存在的seu赋值给他 else: # 不在则计算再加入 max_value = utils.find_max_value(item, items) # 找出最大值 seu = max_value + item['ru'] # 计算seu seu_dic.update(&#123;item['item']: seu&#125;) # 加入到字典中，用于后面的判断 item['seu'] = seu return projected_utility_arraydef ProUM(D, u_table, minsup): \"\"\" 总算法入口函数，可看论文伪代码部分介绍 :param D: 格式化后的数据库 :param u_table: 单位效应数组 :param minsup: 最小支持度 :return: HUSPs: 最终频繁项集（结果） \"\"\" for u in u_table.keys(): swu = SWU(u, D) if swu &lt; minsup: D = utils.delete_item(u, D) utility_array = construct_utility_array(D) pre = &#123;'item': [], 'last_eid': [-1, -1, -1, -1, -1], 'sum_value': [0, 0, 0, 0, 0]&#125; # 初始化空前缀 projected_utility_array = SEU(utility_array) project_search(pre, projected_utility_array, minsup)def project_search(pre, projected_utility_array, minsup): \"\"\" 主递归函数 :param pre: 前缀 :param projected_utility_array:投影数组 :param minsup: 最小支持度 :return:HUSPs \"\"\" copy_pre = copy.deepcopy(pre) i_item = i_concatenation(copy_pre, projected_utility_array) # put I-Concatenation items of t into iItem; for i in i_item.copy(): # remove unpromising items i j ∈ iItem that have SEU(i j) &lt; δ × u(D) if sum(i_item[i]['seu']) &lt; minsup: i_item.pop(i) s_item = s_concatenation(copy_pre, projected_utility_array) # put S-Concatenation items of t into sItem; for s in s_item.copy(): # remove unpromising items i j ∈ sItem that have SEU(i j) &lt; δ × u(D) if sum(s_item[s]['seu']) &lt; minsup: s_item.pop(s) for key, value in i_item.copy().items(): iitem = &#123;key: value&#125; new_pre = i_connect(copy_pre, iitem) SEU_new_pre = sum(copy_pre['sum_value'] + value['seu']) if SEU_new_pre &gt;= minsup: # SEU(t') ≥ δ × u(D) u = sum(copy_pre['sum_value'] + value['u']) new_projected_utility_array = SEU(construct_projected_utility_array(new_pre, projected_utility_array)) if u &gt;= minsup: # u(t') ≥ δ × u(D) HUSPs.update(&#123;str(new_pre['item']): u&#125;) # 加入总结果 project_search(copy.deepcopy(new_pre), new_projected_utility_array, minsup) # 递归查找 for key, value in s_item.copy().items(): sitem = &#123;key: value&#125; new_pre = s_connect(copy_pre, sitem) new_projected_utility_array = SEU(construct_projected_utility_array(new_pre, projected_utility_array)) SEU_new_pre = sum(copy_pre['sum_value'] + value['seu']) if SEU_new_pre &gt;= minsup: # SEU(t') ≥ δ × u(D) u = sum(copy_pre['sum_value'] + value['u']) if u &gt;= minsup: # u(t') ≥ δ × u(D) HUSPs.update(&#123;str(new_pre['item']): u&#125;) # 加入总结果 project_search(new_pre, new_projected_utility_array, minsup) # 递归查找if __name__ == '__main__': D_source = data_set('Database.txt') minsup = 40 # 最小效用阈值 u_table = &#123;'a': 3, 'b': 2, 'c': 10, 'd': 4, 'e': 6, 'f': 1&#125; # 单位效用表 HUSPs = &#123;&#125; ProUM(D_source, u_table, minsup) # HUSPs = utils.set(HUSPs) for item, value in HUSPs.items(): print(item, ':', value)","categories":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/categories/python/"},{"name":"算法","slug":"python/算法","permalink":"https://blog.paomax.com/categories/python/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://blog.paomax.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"ProUM算法","slug":"ProUM算法","date":"2020-05-03T14:32:03.000Z","updated":"2021-02-20T05:08:10.491Z","comments":true,"path":"2020/05/03/2232.html","link":"","permalink":"https://blog.paomax.com/2020/05/03/2232.html","excerpt":"这是一个新的算法，网上除了论文几乎找不到代码资料。这里的代码不完全正确，先做下记录","text":"这是一个新的算法，网上除了论文几乎找不到代码资料。这里的代码不完全正确，先做下记录 ProUm算法 database.txt-数据库文件 12345a[6] c[10] -1 c[20] -1 b[20] f[3] -1 a[6] e[6] -1 -2 SUtility:71f[2] -1 a[15] d[8] -1 c[20] -1 b[8] -1 a[12] d[4] -1 -2 SUtility:69a[12] -1 b[8] -1 f[5] -1 a[3] b[4] e[6] -1 -2 SUtility:38a[9] b[8] d[20] -1 c[20] e[6] -1 -2 SUtility:63b[2] e[6] -1 c[10] -1 f[2] -1 d[8] -1 a[12] e[12] -1 -2 SUtility:52 proUM.py-主入口文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266import utilsdef dataset(filename): D = [] with open(filename, 'r') as input: # 读取文件 for line in input.readlines(): # 一次读取一行 elements = line.replace('[', '=').replace(']', '|').replace(' ', '').split('-1') # 格式化数据 line = [] # 定义空行 for e_1 in elements[0:-1]: # 遍历一行序列，除去最后一个行效应值 # 如果是最后一个数据，即数据库中的SUtility（行总效应值） e_1 = e_1.split('|')[0:-1] # 这里分割最后面一个元素会出现一个空格，所以去掉最后一个空格不要 item = &#123;&#125; # 定义空项 for e_2 in e_1: # 遍历其中一项 e_2 = e_2.split('=') # 分隔项名称和效应值 t = &#123;e_2[0]: int(e_2[1])&#125; # 第二个是=标识符，所以从选择第1个和3个元素，下表是0和2 item.update(t) line.append(item) utility = &#123;'SUtility': int(elements[-1][11:])&#125; # 取出最后一项，得到行效用值 line.append(utility) D.append(line) return D # 返回最终处理后的序列数据库'''构建总的效应数组D：源数据库'''def make_D_utility_array(D): D_utility_array = [] for S in D: utility_array = utils.make_utility_array(S) # 调用生成行效应数组 D_utility_array.append(utility_array) return D_utility_array'''主入口函数D：源数据库u_table：单元效应数组minsup：最小支持度'''def proUM(D, u_table, minsup): D = utils.handle_one_sequences(D, u_table, minsup) D_utility_array = make_D_utility_array(D) project_search([], D_utility_array, minsup)'''主挖掘函数prefix:前缀projected_array：投影数据库minsup：最小支持度'''def project_search(prefix, projected_array, minsup): i_item = i_concatenation(prefix, projected_array) # 得到所有项扩展 i_item = utils.set(i_item) # 去重 s_item = s_concatenation(prefix, projected_array) # 得到所有项扩展 s_item = utils.set(s_item) # 去重 for i in i_item: # 遍历，去掉小于最小效应值的项集 seu = SEU(i, D_source) if seu &lt; minsup: i_item.remove(i) else: HUSPs.append(i) for s in s_item: # 遍历，去掉小于最小效应值的项集 seu = SEU(s, D_source) if seu &lt; minsup: i_item.remove(s) else: HUSPs.append(s) for t in i_item: # 遍历，生成项扩展，递归计算是否满足最下支持度 i = i_concatenation(t, projected_array) for x in i: D_projected_array = utils.make_D_projected_array(x, projected_array) if SEU(x, D_source) &gt;= minsup: HUSPs.append(x) project_search(x, D_projected_array, minsup) for t in s_item: # 遍历，生成序列扩展，递归计算是否满足最下支持度 i = s_concatenation(t, projected_array) for x in i: D_projected_array = utils.make_D_projected_array(x, projected_array) if SEU(x, D_source) &gt;= minsup: HUSPs.append(x) project_search(x, D_projected_array, minsup) return'''项扩展t：序列D_utility_array：生成的序列数据库'''def i_concatenation(t, D_utility_array): if len(t) == 1: pre = [] else: pre = t[:-1] D_projected_array = utils.make_D_projected_array(pre, D_utility_array) if len(t) == 0: e = [] else: e = t[-1] if len(e) &gt; 1: e = e[-1][-1] i = 0 # 下标 i_extend = [] for array in D_projected_array: n = [] for a in array: if a[2] == e: i = a[0] # id j = a[1] # eid for temp in array[i:]: if temp[1] == j: n.append([a[2], temp[2]]) i_extend.append(pre + n) elif len(e) == 0: i_extend.append(pre + [a[2]]) return i_extend'''序列扩展t：序列D_utility_array：生成的序列数据库'''def s_concatenation(t, D_utility_array): if len(t) == 1: pre = [] else: pre = t[:-1] D_projected_array = utils.make_D_projected_array(pre, D_utility_array) #构建投影数据库 if len(t) == 0: e = [] else: e = t[-1] if len(e) &gt; 1: e = e[-1][-1] s_extend = [] for array in D_projected_array: n = [] for a in array: if a[2] == e: i = a[6] if i != '-': temp = array[i - 1][2] x = [e, temp] s_extend.append(pre + x) #加入 elif len(e) == 0: s_extend.append(pre + [a[2]]) return s_extend'''SWU: 宽松约束t: 需要处理的序列，格式有：t=['a']或t=['a','b'] t=[['a','b']] t=[['a','b'],'c']或t=['c',['a','b']]D: 序列数据库'''def SWU(t, D): swu = 0 # 记录行总效应值之和，如s1+s2+s5 for S in D: # 遍历整个序列数据库 sub_nums = &#123;&#125; # 临时，用于调试-下标记录器，用来存储项的下标 i = 0 # 下标记录 flag = 0 # 记录一行内找到的项的个数 for sequences in t: # 遍历的所有项 for s in S[i:]: # 遍历源数据库剩余数据 if set(sequences) &lt;= set(s): # 如果t内项属于当前s项 i = S.index(s) + 1 sub_nums.update(&#123;str(sequences): i&#125;) flag += 1 break if s == None: S.remove(s) if len(t) == flag: # 如果t的长度和fage长度相等，即在一行内找到了t内所有元素 swu += S[-1]['SUtility'] # 将此行的效应值加上 return swu'''SWU: 紧致约束t: 需要处理的序列，格式有：t=['a']或t=['a','b'] t=[['a','b']] t=[['a','b'],'c']或t=['c',['a','b']]D: 序列数据库思路: 先找出所有可能组合的项，然后再尝试组合，把符合要求的提取出来'''def SEU(t, D): seu = 0 # 记录行总效应值之和，如s1+s2+s5 for S in D: # 遍历整个序列数据库 sub_nums = [] # 临时，用于调试-下标记录器，用来存储项的下标 for s in S: # 遍历源数据库剩余数据 for sequences in t: # 遍历的所有项 if set(sequences) &lt;= set(s): # 如果t内的项属于当前s项 sub_nums.append(&#123;str(sequences): S.index(s)&#125;) # 然后添加 t_combs = utils.combine(sub_nums, len(t)) # 获得sub_num的排列 t_chrilds = [] for t_comb in t_combs: # 遍历，过滤无效的组合项 flag = 0 # 标志位 for i in range(len(t_comb)): # 循环比较找到的子序列和被找子序列是否相等 if list(t_comb[i].keys())[0] == str(t[i]): # print(\"aaa\") flag += 1 is_in = True # 标志位 if flag == len(t): # 如果相等 i = 0 for i in range(len(t_comb) - 1): # 遍历找到的可能序列，再判断是否再一个项集里面 if list(t_comb[i].values())[0] == list(t_comb[i + 1].values())[0]: # 只要又一项相等，直接跳出，不判断了，舍去 is_in = False break if i == len(t_comb) - 2 and is_in: # 说明这个子序列完全满足条件 t_chrilds.append(t_comb) # print(S, t_chrilds) # 找u(t,s),行S中的最大效应值 u_t_s = 0 # 表示u(t,s) for t_chrild in t_chrilds: temp = 0 for x in t_chrild: key_1 = list(x.values())[0] # 找到键值1 if ('[' in list(x.keys())[0]): for key_2 in eval(list(x.keys())[0]): temp += S[key_1][key_2] # 计算当前项的效用值 else: key_2 = list(x.keys())[0] # 找到键值2 temp += S[key_1][key_2] # 计算当前项的效用值 if u_t_s &lt; temp: # 赋值u_t_s为最大的那个 u_t_s = temp # 找u(&lt;S-t&gt;)rest,行S中的剩余效应值 u_rest = 0 # 剩余效应值 if len(t_chrilds) &gt;= 1: # 找到的子序列不为空 x = t_chrilds[0] # 先找第一个匹配项 u_rest = utils.rest(S, x) seu += u_t_s + u_rest if len(t) == 1 and len(t[0]) == 1: seu = 99999999 return seuif __name__ == '__main__': D_source = dataset('Database.txt') minsup = 20 # 最小效用阈值 u_table = [&#123;'a': 3&#125;, &#123;'b': 2&#125;, &#123;'c': 10&#125;, &#123;'d': 4&#125;, &#123;'e': 6&#125;, &#123;'f': 1&#125;] # 单位效用表 HUSPs = [] proUM(D_source, u_table, minsup) HUSPs = utils.set(HUSPs) # 去重 for i in range(len(HUSPs)): print(HUSPs[i]) utils-工具类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173import copyimport proUM'''一个递归查找序列排列的函数list:需要排列的列表l:每个排列的项数'''#可不必在意def combine(list, l): result = [] tmp = [0] * l length = len(list) def next_num(li=0, ni=0): if ni == l: result.append(copy.copy(tmp)) return for lj in range(li, length): tmp[ni] = list[lj] next_num(lj + 1, ni + 1) next_num() return result'''查找剩余效应值由所在项的之后的其他元素的效应值加上此项之后的效应值构成S:行序列s:需要查找剩余效应值的序列'''def rest(S, s): rest = 0 # 剩余效应值 s = s[-1] # 取最后一项 key_1 = list(s.values())[0] # 找到键值1 # 找u(&lt;S-t&gt;)rest,行S中的剩余效应值 # 先找第一个匹配项 if ('[' in list(s.keys())[0]): # 带‘[’说明键值大于两项，如&lt;(a,b)&gt; i = eval(list(s.keys())[0])[-1] # 找到如&lt;(a,b)&gt;的最后一个b的下标 keys = list(S[key_1].keys()) m = keys[keys.index(i) + 1:] for key_2 in m: rest += S[key_1][key_2] # 计算当前项的效用值 else: keys = list(S[key_1].keys()) i = list(s.keys())[0] m = keys[keys.index(i) + 1:] for key_2 in m: rest += S[key_1][key_2] # 计算当前项的效用值 for s in S[key_1 + 1:len(S) - 1]: # 去掉最后一个项的行总效应值项 rest += sum(s.values()) return rest'''整理数据库，去掉不满足条件的一频繁项集D:源数据库u_table:单位效用表minsup:最小支持度'''def handle_one_sequences(D, u_table, minsup): for u in u_table: # 计算一项集SWU e = list(u.keys()) swu = proUM.SWU(e, D) if swu &lt; minsup: # 如果小于则去除 for S in D: for s in S: if set(e) &lt;= set(list(s.keys())): # 如果在里面就删除 S[-1]['SUtility'] = int(list(S[-1].values())[0]) - s[e[0]] s.pop(e[0]) if len(s) == 0: S.remove(s) return D'''建立一行的效应数组:'''def make_utility_array(S): i = 0 # 计数 utility_array = [] # 临时array for s in S: # 遍历数据库行 eid = S.index(s) # eid for item, u in s.items(): i += 1 # array id ru = rest(S, [&#123;item: eid&#125;]) utility_array.append([i, eid, item, u, ru]) utility_array = utility_array[:-1] for i in range(len(utility_array)): # 遍历查找next_position,next_eid for j in range(i + 1, len(utility_array)): # 找next_position if utility_array[i][2] == utility_array[j][2]: # 如果元素相同，说明找到 utility_array[i].append(utility_array[j][0]) # 则将找到元素的index赋值给next_position break elif j == len(utility_array) - 1: # 找到最后没找到 utility_array[i].append('-') if i == len(utility_array) - 1: # 最后一个元素 utility_array[i].append('-') for j in range(i + 1, len(utility_array)): # 找next_eid if utility_array[i][1] != utility_array[j][1]: # 如果元素相同，说明找到 utility_array[i].append(utility_array[j][0]) # 则将找到元素的index赋值给next_position break elif j == len(utility_array) - 1: # 找到最后没找到 utility_array[i].append('-') if i == len(utility_array) - 1: # 最后一个元素 utility_array[i].append('-') return utility_array'''构建一行的投影数据库'''def make_projected_array(t, utility_array): if len(t) &gt; 1: e = t[-1] else: e = t if len(e) == 1 and len(e[0]) != 1: e = e[-1][-1] i = 0 # 下标 for a in utility_array: i = a[6] if a[2] == e: if i == '-': utility_array = [] break else: i = a[6] - 1 break if i != '-': utility_array = utility_array[i:] return utility_array'''构建整个某个序列的投影数据库t:序列D_utility_array：数据库总的效应数组'''def make_D_projected_array(t, D_utility_array): D_projected_array = [] for array in D_utility_array: projected_array = make_projected_array(t, array) #调用行投影数据库函数 D_projected_array.append(projected_array) #加入 return D_projected_array'''去重函数'''def set(list): new_list = [] for id in list: if id not in new_list: new_list.append(id) return new_list","categories":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/categories/python/"},{"name":"算法","slug":"python/算法","permalink":"https://blog.paomax.com/categories/python/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://blog.paomax.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"FP-growth算法","slug":"FP-growth算法","date":"2020-04-09T11:06:03.000Z","updated":"2021-02-20T05:08:10.490Z","comments":true,"path":"2020/04/09/1906.html","link":"","permalink":"https://blog.paomax.com/2020/04/09/1906.html","excerpt":"","text":"FP-growt算法 代码参考了这篇博客文章 算法具体内容网上一大堆，这个代码写的很好，已经注释的很详细了。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240# 数据库函数，包含待处理的数据，数据格式为 列表def loadDataSet(): # ## 测试数据1 # dataSet = [['bread', 'milk', 'vegetable', 'fruit', 'eggs'], # ['noodle', 'beef', 'pork', 'water', 'socks', 'gloves', 'shoes', 'rice'], # ['socks', 'gloves'], # ['bread', 'milk', 'shoes', 'socks', 'eggs'], # ['socks', 'shoes', 'sweater', 'cap', 'milk', 'vegetable', 'gloves'], # ['eggs', 'bread', 'milk', 'fish', 'crab', 'shrimp', 'rice']] ##测试数据2 dataSet = [ ['f', 'a', 'c', 'd', 'g', 'i', 'm', 'p'], ['a', 'b', 'c', 'f', 'l', 'm', 'o'], ['b', 'f', 'h', 'j', 'o'], ['b', 'c', 'k', 's', 'p'], ['a', 'f', 'c', 'e', 'l', 'p', 'm', 'n'] ] return dataSet# 数据格式转换函数def transfer2FrozenDataSet(dataSet): frozenDataSet = &#123;&#125; ##定义一个空的字典数据 for elem in dataSet: #遍历数据库 frozenDataSet[frozenset(elem)] = 1 #把数据库每行的支持度赋值为 1，frozenset()函数是python内置函数，返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。 return frozenDataSet #返回处理后的数据''' 首先，需要创建一个树形的数据结构，叫做 FP 树。该树结构包含结点名称 nodeName，结点元 素出现频数 count，父节点 nodeParent，指向下一个相同元素的指针 nextSimilarItem，子节点集合 children。'''## FP树节点类class TreeNode: # 初始化函数 def __init__(self, nodeName, count, nodeParent): self.nodeName = nodeName #节点名 self.count = count #节点支持度 self.nodeParent = nodeParent #父节点名 self.nextSimilarItem = None #子节点名 self.children = &#123;&#125; #用字典存储该节点所有子节点信息 # 支持度加 1 函数，用在构建FP-Tree时加入新数据行时使用 def increaseC(self, count): self.count += count ##当前支持度加 1 ## 打印 FP-Tree信息函数，了解即可，此处可跳过 def disp(self, ind=1): print(' '*ind, self.nodeName, ' ', self.count) for child in self.children.values(): child.disp(ind+1)''' --------------------------------------------------------------------------------------------------------------- 1.创建FP-Tree 此步包括： （1）项头表的建立； （2）FP-Tree的建立； --------------------------------------------------------------------------------------------------------------- 用第一步构造出的数据结构来创建 FP 树。 下面代码主要分为两层: 第一层，扫描数据库，统计出各个元素的出现频数。将数据记录中不包含在频繁元素中的元素删除，创建项头表； 第二层，扫描数据库，对每一条数据记录，，然后将数据记 录中的元素按出现频数排序。将数据记录逐条插入 FP 树中，不断更新 FP 树，更新的过程会在后面介绍。'''# 创建树函数def createFPTree(frozenDataSet, minSupport): #第一次扫描数据库, 统计出各个元素的出现次数，过滤出支持度小于最小支持度的元素，生成项头表 headPointTable = &#123;&#125; #定义空项头表 for items in frozenDataSet: #遍历处理后的数据库 for item in items: headPointTable[item] = headPointTable.get(item, 0) + frozenDataSet[items] #计算单个元素的支持度，dict.get()函数返回指定键的值 headPointTable = &#123;k:v for k,v in headPointTable.items() if v &gt;= minSupport&#125; #删除支持度小于最小支持度的元素 frequentItems = set(headPointTable.keys()) # frequentItems为所有支持度大于最小支持度的元素，##set() 函数创建一个无序不重复元素集，可进行关系测试，删除重复数据，还可以计算交集、差集、并集等。 if len(frequentItems) == 0: return None, None #如果没有符合最小支持度的元素，则返回空 for k in headPointTable: #遍历头指针列表 headPointTable[k] = [headPointTable[k], None] #把头列表的['h':3]格式变成['h':[3,NONE]]格式 fptree = TreeNode(\"null\", 1, None) #定义空FP-Tree #第二次扫描数据库, 将数据记录中的元素按出现频数排序 for items, count in frozenDataSet.items(): #items为dataSet的一行记录值，count为这一行记录值出现的次数 frequentItemsInRecord = &#123;&#125; #定义空的字典，保存数据库某行的大于最小支持度的所有元素，充当临时变量 for item in items: #遍历某一行所有数据库元素 if item in frequentItems: #如果某一元素在频繁一项集里 frequentItemsInRecord[item] = headPointTable[item][0] #则把这项赋值给frequentItemsInRecord if len(frequentItemsInRecord) &gt; 0: #如果某数据库一行有大于最小支持度的元素 #则对元素按照出现次数进行排序 ##下面一行时python生成器的写法，主要作用时对某一行元素排序 orderedFrequentItems = [v[0] for v in sorted(frequentItemsInRecord.items(), key=lambda v:v[1], reverse = True)] ##key 主要是用来进行比较的元素，这里用支持度排序 updateFPTree(fptree, orderedFrequentItems, headPointTable, count) #更新树节点 ## 进入 updateFPTree（） return fptree, headPointTable ##返回最终树和头指针列表''' 下面函数主要用来更新 FP 树，这里用到了递归的技巧。每次递归迭代中，处理数据记录中的第一个元素处理，如果该元素 是 fptree 节点的子节点，则只增加该子节点的 count 树，否则，需要新创建一个 TreeNode 节点，然后将其 赋给 fptree 节点的子节点，并更新项头表关于下一个相同元素指针的信息。再把除第一个元素之外的元素进行递归。 迭代的停止条件是当前迭代的数据记录长度小于等于 1。'''def updateFPTree(fptree, orderedFrequentItems, headPointTable, count): #处理（##删除小于最小支持度元素后的）数据库第一行的数据，如果第一个节点在已经存在之前的树中，则给相应的子树执行支持度加1操作。 if orderedFrequentItems[0] in fptree.children: ## 如果在子节点中 fptree.children[orderedFrequentItems[0]].increaseC(count) ## 加1 # 如果第一个节点不在任何一个已存在子树中，则增加分支 else: fptree.children[orderedFrequentItems[0]] = TreeNode(orderedFrequentItems[0], count, fptree) # 把当前第一个节点加到上一节点的子节点中（因为是递归，所以这样做不是只针对root的操作） # 如果相应的头指针值没有指向节点（即值为None），则指向当前处理的这个节点 if headPointTable[orderedFrequentItems[0]][1] == None: ## 相应的头指针值为None headPointTable[orderedFrequentItems[0]][1] = fptree.children[orderedFrequentItems[0]] ## 指向当前处理的这个节点 # 如果头列表里面不为空，则进入updateHeadPointTable函数，查找已存在的相同名称节点且没有指向任何节点的节点，然后让其指向此节点 else: updateHeadPointTable(headPointTable[orderedFrequentItems[0]][1], fptree.children[orderedFrequentItems[0]]) ## 进入updateHeadPointTable（） #如果数据个数大于1个，则处理除第一个之外的节点，递归实现，即删除初第一个元素之外，把其他元素再进行上面的操作 if(len(orderedFrequentItems) &gt; 1): updateFPTree(fptree.children[orderedFrequentItems[0]], orderedFrequentItems[1::], headPointTable, count) ##层层深入递归，实在不好理解，知道大体意思即可''' 更新头指针函数'''def updateHeadPointTable(headPointBeginNode, targetNode): while(headPointBeginNode.nextSimilarItem != None): # 循环下一个节点，直到找到没有指向任何节点的指针 headPointBeginNode = headPointBeginNode.nextSimilarItem headPointBeginNode.nextSimilarItem = targetNode # 然后指向目标节点''' --------------------------------------------------------------------------------------------------------------- 2.从FP树中挖掘频繁项集 此步包括： （1）从FP-Tree中获得条件模式基； （2）利用条件模式基，构建一个条件FP-Tree； （3）迭代重复（1）（2）直到树包含一个元素项为止 ---------------------------------------------------------------------------------------------------------------下面开始挖掘频繁项集，这里也是递归迭代的思路。对于项头表中的每一个元素，首先获取该元素结尾的所有前缀路径，然后将所有前缀路径作为新的数据集传入 createFPTree 函数中以创建条件 FP 树。然后对条件 FP 树对应的项头表中的每一个元素，开始获取前缀路径，并创建新的条件 FP 树。这两步不断重复，直到条件 FP 树中只有一个元素为止。'''def mineFPTree(headPointTable, prefix, frequentPatterns, minSupport): # 对于headPointTable中的每个项，查找条件前缀路径，创建条件fptree，然后迭代，直到条件fptree中只有一个元素 headPointItems = [v[0] for v in sorted(headPointTable.items(), key = lambda v:v[1][0])] # 先对头列表按支持度降序排序，因为要从最底下支持度最小的元素节点开始向上处理 if(len(headPointItems) == 0): return #如果项头表为空集，则返回，即直到树包含一个元素项为止 for headPointItem in headPointItems: #遍历项头表 newPrefix = prefix.copy() # 复制一份前缀 newPrefix.add(headPointItem) # 将当前节点加入到前缀路径中，此处用于递归操作 support = headPointTable[headPointItem][0] # 取出项头表某元素的支持度 frequentPatterns[frozenset(newPrefix)] = support # frequentPatterns为记录频繁项集的变量。## 并把前缀路径的支持度赋值为当前寻找前缀路径元素的支持度，比如a和b的频数为3，是因为c的频数为 2，所以与c共同出现的a和b的频数就都为2 ##frozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。 prefixPath = getPrefixPath(headPointTable, headPointItem) # 获取前缀路径 ## 进入getPrefixPath函数 if(prefixPath != &#123;&#125;): # 前缀路径不为空时 ## conditionalFPtree ：条件FP树，conditionalHeadPointTable：条件项头表 conditionalFPtree, conditionalHeadPointTable = createFPTree(prefixPath, minSupport) #则构建条件FP-Tree if conditionalHeadPointTable != None: # 当项头表不为空则继续递归 mineFPTree(conditionalHeadPointTable, newPrefix, frequentPatterns, minSupport) #递归处理条件FP-Tree''' 获取所有前缀路径。对于每一个相同元素，通过父节点指针不断向上遍历，所得的路径就是该元素的前缀路径。'''def getPrefixPath(headPointTable, headPointItem): prefixPath = &#123;&#125; #定义空前缀路径 beginNode = headPointTable[headPointItem][1] #开头节点为函数传入的节点 prefixs = ascendTree(beginNode) # 获取其中一个节点的前缀路径的所有节点名 ## 进入ascendTree（） if((prefixs != [])): #如果前缀节点不为空 prefixPath[frozenset(prefixs)] = beginNode.count # 使前缀路径的支持度赋值为开始节点的支持度，比如d路径的子路径a→b→c→d，因为a→b→c支持度为3，d节点支持度为2，则a→b→c支持度赋值为2 while(beginNode.nextSimilarItem != None): #循环找出剩余的相同名称节点的前缀路径，原理与上面相同 beginNode = beginNode.nextSimilarItem prefixs = ascendTree(beginNode) if (prefixs != []): prefixPath[frozenset(prefixs)] = beginNode.count return prefixPath #返回前缀节点''' 获取其中一个节点的前缀路径的所有节点名'''def ascendTree(treeNode): prefixs = [] # 定义空前缀名列表 while((treeNode.nodeParent != None) and (treeNode.nodeParent.nodeName != 'null')): #一直循环知道无前缀节点 treeNode = treeNode.nodeParent #取前缀节点名 prefixs.append(treeNode.nodeName) #把前缀节点名加入perfixs变量中保存。 return prefixs #返回前缀节点名'''------------------------------------------------------------------------------------------------------------------------------------------------------3.下面是挖掘关联规则------------------------------------------------------------------------------------------------------------------------------------------------------关联规则通俗的讲就是一个事件发生的前提下另一个事件发生的概率，比如这个算法最初的问题出发点：一个人买了尿布后，有多大可能会再买啤酒此事件的计算公式为：两个事件同时发生的概率 ÷ 第一个事件发生的概率关联规则挖掘首先需要对上文得到的频繁项集构建所有可能的规则，然后对每条规则逐个计算置信度，输出置信度大于最小置信度的所有规则。以频繁项集&#123;a,b,c&#125;为例，构建所有可能的规则：&#123;b,c&#125; -&gt; &#123;a&#125;, &#123;a,c&#125; -&gt; &#123;b&#125;,&#123;a,b&#125; -&gt; &#123;c&#125;,&#123;c&#125; -&gt; &#123;a,b&#125;,&#123;b&#125; -&gt; &#123;a,c&#125;,&#123;a&#125; -&gt; &#123;b,c&#125;。对每条规则计算置信度后，输出满足要求的规则即可。'''#关联规则入口函数def rulesGenerator(frequentPatterns, minConf, rules): for frequentset in frequentPatterns: #遍历频繁项集，找出大于1个元素的频繁项集进行关联规则运算 if(len(frequentset) &gt; 1): getRules(frequentset,frequentset, rules, frequentPatterns, minConf) ##进入getRules（），getRules（）才算是查找关联规则的主体函数#计算子集的函数def removeStr(set, str): tempSet = [] ## 定义空列表 for elem in set: # 从set中去除和str相同的元素，得到set的一个子集 if(elem != str): tempSet.append(elem) tempFrozenSet = frozenset(tempSet) return tempFrozenSet #返回这个子集def getRules(frequentset,currentset, rules, frequentPatterns, minConf): for frequentElem in currentset: #因为是递归，随意是遍历当前集合，此集合会随着递归的深入而变化 subSet = removeStr(currentset, frequentElem) #找子集 confidence = frequentPatterns[frequentset] / frequentPatterns[subSet] #多个事件同时发生的概率（买尿布和啤酒） ÷ 前因发生的概率（买啤酒） if (confidence &gt;= minConf): #算出的置信度大于等于最小置信度 flag = False #标志位赋值为False for rule in rules: # 遍历关联规则 if(rule[0] == subSet and rule[1] == frequentset - subSet): #如果当前关联规则已经存在 flag = True #标志位赋值为True，也就是不把当前规则加入到规则列表中 if(flag == False): #如果标志位为False，则把当前规则加入到规则列表 rules.append((subSet, frequentset - subSet, confidence)) #如果还有子集，则递归计算剩余子集置信度 if(len(subSet) &gt;= 2): getRules(frequentset, subSet, rules, frequentPatterns, minConf)if __name__=='__main__': print(\"fptree:\") ##打印FP-Tree结构，此处可跳过 dataSet = loadDataSet() #加载数据 frozenDataSet = transfer2FrozenDataSet(dataSet) #处理数据 minSupport = 3 #定义最小支持度 fptree, headPointTable = createFPTree(frozenDataSet, minSupport) #创建FP-Tree fptree.disp() ##打印FP-Tree结构，此处可跳过 frequentPatterns = &#123;&#125; #定义空频繁项集 prefix = set([]) #定义空前缀路径 mineFPTree(headPointTable, prefix, frequentPatterns, minSupport) #挖掘频繁项集 print(\"frequent patterns:\") print(frequentPatterns) #打印频繁项集 minConf = 0.6 #定义置信度 rules = [] ##定义空关联规则 rulesGenerator(frequentPatterns, minConf, rules) #挖掘关联规则 print(\"association rules:\") #打印关联规则 print(rules)","categories":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/categories/python/"},{"name":"算法","slug":"python/算法","permalink":"https://blog.paomax.com/categories/python/%E7%AE%97%E6%B3%95/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://blog.paomax.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"淘宝登录密码RSA加密分析及相关代码的python实现","slug":"淘宝登录密码加密分析及相关代码的python实现","date":"2020-03-04T08:00:48.000Z","updated":"2021-02-20T05:08:10.492Z","comments":true,"path":"2020/03/04/1600.html","link":"","permalink":"https://blog.paomax.com/2020/03/04/1600.html","excerpt":"这个特殊时期中，口罩难买。为了能在网上抢购口罩，就去找各种自动枪口罩的脚本，但是功能都不尽人意，于是想要自己写一个。这里是记录的一个关于淘宝登录的问题。","text":"这个特殊时期中，口罩难买。为了能在网上抢购口罩，就去找各种自动枪口罩的脚本，但是功能都不尽人意，于是想要自己写一个。这里是记录的一个关于淘宝登录的问题。 其实对于这种网页http请求的抓包，很久之前就很感兴趣，但一直以为很难，也没时间搞，毕竟是破解别人的东西。但通过两天的研究，也不是想的那么难，而且破解的还是淘宝，虽然这种技术对大牛来说小菜一碟，但好久没有这种成就感了，嘿嘿😁！下面就分享我的学习过程 一、淘宝http请求分析 参考博客: https://www.jianshu.com/p/f0b3ea504acf 1. 首先说一下Chrome和Firefox两者的开发者工具区别 Chrome 优点：更底层，分析的更彻底 缺点：全英文，对中国人不太友好；使用起来较复杂；一些跳转页面找不到具体的请求 Firefox 优点：界面更好看；几乎全中文界面，跳转页面代码也能很容易看到 缺点：对追究细节的开发者不友好；network栏目不能格式化js 总体来说，我是喜欢Firefox，感觉它更是一种现代化的工具。 2. 打开淘宝登录页面，按F12打开Firefox开发者工具，找到网络选项。等待抓包。输入账号密码，用户名最好正确输入，密码随便输入即可，然后点击登录按钮，开始抓包。 会看到参数里面有个TPL_password_2,这个直接全局查找TPL_password_2关键词还不一定能找得到，在搜索TPL_password_2关键词没有结果后就尝试搜了了搜其他关键词如RSA、encrypt，尤其是encrypt，还真找到了对应的加密js文件： 分析里面的代码可以找到this.exponent是RSA加密指数，值是10001、this.pbk是密钥模数其值是网页源码的一个元素值。 好的，到这里分析工作基本就完成了 二、利用模数和指数生成密钥 python代码，需要安装cryptography模块 123456789101112131415161718192021222324252627282930313233from cryptography.hazmat.backends import default_backendfrom cryptography.hazmat.primitives.asymmetric import rsafrom cryptography.hazmat.primitives import serializationdef populate_public_key(rsaExponent, rsaModulus): ''' 根据cryptography包下的rsa模块，对指数模数进行处理生成公钥 :param rsaExponent:指数 :param rsaModulus:模数 :return:公钥 ''' rsaExponent = int(rsaExponent, 16) # 十六进制字符串转十进制 rsaModulus = int(rsaModulus, 16) pubkey = rsa.RSAPublicNumbers(rsaExponent, rsaModulus).public_key(default_backend()) return pubkeydef save_pub_key(pub_key, pem_name): #将公钥编码为PEM格式的数据 pem = pub_key.public_bytes( encoding=serialization.Encoding.PEM, format=serialization.PublicFormat.SubjectPublicKeyInfo ) # 将PEM个数的数据写入文本文件中 with open(pem_name, 'w+') as f: f.writelines(pem.decode())if __name__ == '__main__': rsaExponent = \"010001\" #指数 #淘宝的J_PBK元素的值 rsaModulus = \"9a39c3fefeadf3d194850ef3a1d707dfa7bec0609a60bfcc7fe4ce2c615908b9599c8911e800aff684f804413324dc6d9f982f437e95ad60327d221a00a2575324263477e4f6a15e3b56a315e0434266e092b2dd5a496d109cb15875256c73a2f0237c5332de28388693c643c8764f137e28e8220437f05b7659f58c4df94685\" pubkey = populate_public_key(rsaExponent, rsaModulus) save_pub_key(pubkey,\"pbk.pem\") 三、利用密钥加密密码 从上面图中的参数TPL_password_2看到这些值是16进制的，所以一般的吧加密后的密文输出为base64（64进制）是不行的，所以保存成16进制。这个地方弄了好长时间，基础还是太差了😥。为了起到警示作用，我把错误代码注释掉一起放到下面的代码中。 1234567891011121314151617181920import rsaimport base64from urllib import requestimport binasciidef encrypt(): # 用公钥加密 with open('success.pem', 'rb') as publickfile: p = publickfile.read() pubkey = rsa.PublicKey.load_pkcs1_openssl_pem(p) #pubkey = rsa.PublicKey.load_pkcs1(p) #这一个代码会报错【No PEM start marker \"b'-----BEGIN RSA PUBLIC KEY-----'\" found】，百度后说是公钥格式的问题，先放一放 original_text = 'the password'.encode('utf8') code = rsa.encrypt(original_text, pubkey) #下面是未转换时候的字节值 #code = b'\\x1a~\\x1fu\\x9a\\x11\\xe3\\xf2\\xa1\\xcc\\x16\\xe6\\xdb(1m\\x05@a,\\xa2A\\x00j\\x19\\xa7\\xe2\\xad\\xc1W \\x8bN\\x96E&#123;H\\xf3C\\xcdB\\xa7\\xf1\\xd0;\\x8b\\x9a\\xd4/\\xaeY\\xa5D\\xa4\\xfb\\x0c\\xf0j.Y\\xa0\\x86\\r\\x84,\\xee\\xdc\\x92\\xb1RO\\xb1\\x9fW\\xc2&#125;\\xa4\\xbf\\xb4f\\x91\\x19\\xc2\\xe6\\xe6\\xd1n\\x07Z\\xc1\\xe1^(]\\xd9\\x94\\xb7Y\\xf2;\\n:\\x17ela\\xa1\\xac:2P\\xb5\\xdd\\x8d\\xda\\x13\\xae\\xcf0\\xcbM\\x82)\\xf5_9\\x10\\x99' code = str(binascii.b2a_hex(code))[2:-1] #code = base64.b64encode(code).decode('utf-8') #之前的转换成64进制 #code = request.quote(code) return codeprint(encrypt()) 一段生成的密文，和淘宝生成的长度一样，而且通过了登录测试。 1594cfa7f94e7224ad24b4e54ceaa8159c2b4076cec6396a47a90f962431a6f58827c6b0a33970f159b23662d394ef62757e680ea8fadf0bbf3634179a3cbfebc358a095b1cd9e8a01885e46c6fe3d792d8799e82ef82c35dd349b7943ccaee0e16be50340c98673e6d12e63ff5f4fd7d5f6ddcab99e50b888c2e037a9ee10901 注意第十行这个点，以后再来补充 一个小知识点：不要在同一目录下创建和调用的全局模块文件名相同的的文件，因为会优先调用当前目录下的文件 登录代码以后再分析，来自他人，先放到下面 参考博客：https://blog.csdn.net/u014044812/article/details/99584382 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307import reimport osimport jsonimport requests\"\"\"获取详细教程、获取代码帮助、提出意见建议关注微信公众号「裸睡的猪」与猪哥联系@Author : 猪哥,@Version : 2.0\"\"\"\"s = requests.Session()# cookies序列化文件COOKIES_FILE_PATH = 'taobao_login_cookies.txt'class UsernameLogin: def __init__(self, username, ua, TPL_password2): \"\"\" 账号登录对象 :param username: 用户名 :param ua: 淘宝的ua参数 :param TPL_password2: 加密后的密码 \"\"\" # 检测是否需要验证码的URL self.user_check_url = 'https://login.taobao.com/member/request_nick_check.do?_input_charset=utf-8' # 验证淘宝用户名密码URL self.verify_password_url = \"https://login.taobao.com/member/login.jhtml\" # 访问st码URL self.vst_url = 'https://login.taobao.com/member/vst.htm?st=&#123;&#125;' # 淘宝个人 主页 self.my_taobao_url = 'http://i.taobao.com/my_taobao.htm' # 淘宝用户名 self.username = username # 淘宝关键参数，包含用户浏览器等一些信息，很多地方会使用，从浏览器或抓包工具中复制，可重复使用 self.ua = ua # 加密后的密码，从浏览器或抓包工具中复制，可重复使用 self.TPL_password2 = TPL_password2 # 请求超时时间 self.timeout = 3 def _user_check(self): \"\"\" 检测账号是否需要验证码 :return: \"\"\" data = &#123; 'username': self.username, 'ua': self.ua &#125; try: response = s.post(self.user_check_url, data=data, timeout=self.timeout) response.raise_for_status() except Exception as e: print('检测是否需要验证码请求失败，原因：') raise e needcode = response.json()['needcode'] print('是否需要滑块验证：&#123;&#125;'.format(needcode)) return needcode def _verify_password(self): \"\"\" 验证用户名密码，并获取st码申请URL :return: 验证成功返回st码申请地址 \"\"\" verify_password_headers = &#123; 'Connection': 'keep-alive', 'Cache-Control': 'max-age=0', 'Origin': 'https://login.taobao.com', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36', 'Content-Type': 'application/x-www-form-urlencoded', 'Referer': 'https://login.taobao.com/member/login.jhtml?from=taobaoindex&amp;f=top&amp;style=&amp;sub=true&amp;redirect_url=https%3A%2F%2Fi.taobao.com%2Fmy_taobao.htm', &#125; # 登录toabao.com提交的数据，如果登录失败，可以从浏览器复制你的form data verify_password_data = &#123; 'TPL_username': self.username, 'ncoToken': 'cdf05a89ad5104403ebb12ebc9b7626af277b066', 'slideCodeShow': 'false', 'useMobile': 'false', 'lang': 'zh_CN', 'loginsite': 0, 'newlogin': 0, 'TPL_redirect_url': 'https://s.taobao.com/search?q=%E9%80%9F%E5%BA%A6%E9%80%9F%E5%BA%A6&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.2017.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170306', 'from': 'tb', 'fc': 'default', 'style': 'default', 'keyLogin': 'false', 'qrLogin': 'true', 'newMini': 'false', 'newMini2': 'false', 'loginType': '3', 'gvfdcname': '10', 'gvfdcre': '68747470733A2F2F6C6F67696E2E74616F62616F2E636F6D2F6D656D6265722F6C6F676F75742E6A68746D6C3F73706D3D61323330722E312E3735343839343433372E372E33353836363032633279704A767526663D746F70266F75743D7472756526726564697265637455524C3D6874747073253341253246253246732E74616F62616F2E636F6D25324673656172636825334671253344253235453925323538302532353946253235453525323542412532354136253235453925323538302532353946253235453525323542412532354136253236696D6766696C65253344253236636F6D6D656E64253344616C6C2532367373696425334473352D652532367365617263685F747970652533446974656D253236736F75726365496425334474622E696E64657825323673706D253344613231626F2E323031372E3230313835362D74616F62616F2D6974656D2E31253236696525334475746638253236696E69746961746976655F69642533447462696E6465787A5F3230313730333036', 'TPL_password_2': self.TPL_password2, 'loginASR': '1', 'loginASRSuc': '1', 'oslanguage': 'zh-CN', 'sr': '1440*900', 'osVer': 'macos|10.145', 'naviVer': 'chrome|76.038091', 'osACN': 'Mozilla', 'osAV': '5.0 (Macintosh; Intel Mac OS X 10_14_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36', 'osPF': 'MacIntel', 'appkey': '00000000', 'mobileLoginLink': 'https://login.taobao.com/member/login.jhtml?redirectURL=https://s.taobao.com/search?q=%E9%80%9F%E5%BA%A6%E9%80%9F%E5%BA%A6&amp;imgfile=&amp;commend=all&amp;ssid=s5-e&amp;search_type=item&amp;sourceId=tb.index&amp;spm=a21bo.2017.201856-taobao-item.1&amp;ie=utf8&amp;initiative_id=tbindexz_20170306&amp;useMobile=true', 'showAssistantLink': '', 'um_token': 'T898C0FDF1A3CEE5389D682340C5F299FFE590F51543C8E3DDA8341C869', 'ua': self.ua &#125; try: response = s.post(self.verify_password_url, headers=verify_password_headers, data=verify_password_data, timeout=self.timeout) response.raise_for_status() # 从返回的页面中提取申请st码地址 except Exception as e: print('验证用户名和密码请求失败，原因：') raise e # 提取申请st码url apply_st_url_match = re.search(r'&lt;script src=\"(.*?)\"&gt;&lt;/script&gt;', response.text) # 存在则返回 if apply_st_url_match: print('验证用户名密码成功，st码申请地址：&#123;&#125;'.format(apply_st_url_match.group(1))) return apply_st_url_match.group(1) else: raise RuntimeError('用户名密码验证失败！response：&#123;&#125;'.format(response.text)) def _apply_st(self): \"\"\" 申请st码 :return: st码 \"\"\" apply_st_url = self._verify_password() try: response = s.get(apply_st_url) response.raise_for_status() except Exception as e: print('申请st码请求失败，原因：') raise e st_match = re.search(r'\"data\":&#123;\"st\":\"(.*?)\"&#125;', response.text) if st_match: print('获取st码成功，st码：&#123;&#125;'.format(st_match.group(1))) return st_match.group(1) else: raise RuntimeError('获取st码失败！response：&#123;&#125;'.format(response.text)) def login(self): \"\"\" 使用st码登录 :return: \"\"\" # 加载cookies文件 if self._load_cookies(): return True # 判断是否需要滑块验证 self._user_check() st = self._apply_st() headers = &#123; 'Host': 'login.taobao.com', 'Connection': 'Keep-Alive', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36' &#125; try: response = s.get(self.vst_url.format(st), headers=headers) response.raise_for_status() except Exception as e: print('st码登录请求，原因：') raise e # 登录成功，提取跳转淘宝用户主页url my_taobao_match = re.search(r'top.location.href = \"(.*?)\"', response.text) if my_taobao_match: print('登录淘宝成功，跳转链接：&#123;&#125;'.format(my_taobao_match.group(1))) self._serialization_cookies() return True else: raise RuntimeError('登录失败！response：&#123;&#125;'.format(response.text)) def _load_cookies(self): # 1、判断cookies序列化文件是否存在 if not os.path.exists(COOKIES_FILE_PATH): return False # 2、加载cookies s.cookies = self._deserialization_cookies() # 3、判断cookies是否过期 try: self.get_taobao_nick_name() except Exception as e: os.remove(COOKIES_FILE_PATH) print('cookies过期，删除cookies文件！') return False print('加载淘宝登录cookies成功!!!') return True def _serialization_cookies(self): \"\"\" 序列化cookies :return: \"\"\" cookies_dict = requests.utils.dict_from_cookiejar(s.cookies) with open('tempc.txt', 'w+', encoding='utf-8') as file: for i in cookies_dict: file.write(i+':'+cookies_dict[i]+'; ') with open(COOKIES_FILE_PATH, 'w+', encoding='utf-8') as file: json.dump(cookies_dict, file) print('保存cookies文件成功！') def _deserialization_cookies(self): \"\"\" 反序列化cookies :return: \"\"\" with open(COOKIES_FILE_PATH, 'r+', encoding='utf-8') as file: cookies_dict = json.load(file) cookies = requests.utils.cookiejar_from_dict(cookies_dict) return cookies def get_taobao_nick_name(self): \"\"\" 获取淘宝昵称 :return: 淘宝昵称 \"\"\" headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36' &#125; try: response = s.get(self.my_taobao_url, headers=headers) response.raise_for_status() except Exception as e: print('获取淘宝主页请求失败！原因：') raise e # 提取淘宝昵称 nick_name_match = re.search(r'&lt;input id=\"mtb-nickname\" type=\"hidden\" value=\"(.*?)\"/&gt;', response.text) if nick_name_match: print('登录淘宝成功，你的用户名是：&#123;&#125;'.format(nick_name_match.group(1))) return nick_name_match.group(1) else: raise RuntimeError('获取淘宝昵称失败！response：&#123;&#125;'.format(response.text)) def order(self): url = \"https://buy.taobao.com/auction/buy_now.jhtml?spm=2013.1.20140002.d1.undefined\" s.cookies = self._deserialization_cookies() headers = &#123; 'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', #'accept-encoding': ' gzip, deflate, br', 'accept-language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7', 'cache-control': 'max-age=0', 'content-length': '762', 'content-type': 'application/x-www-form-urlencoded', #'cookie': 'XSRF-TOKEN:bce57316-fd8f-4a80-b38e-6c97169750fd; _cc_:U%2BGCWk%2F7og%3D%3D; _hvn_login:0; _l_g_:Ug%3D%3D; _nk_:%5Cu9E21%5Cu722A%5Cu91CA%5Cu5C0A; _sameste_flag_:true; _tb_token_:fb5977336131; cookie1:BqeFozsAR2aqi6ZeWHnKPSq%2Bkv78m6zi5oA8CN5%2BRp8%3D; cookie17:UUwU26AEyM8whA%3D%3D; cookie2:120746cea78eb2f1afc31c71040154a; csg:07feb9d1; dnk:%5Cu9E21%5Cu722A%5Cu91CA%5Cu5C0A; existShop:MTU4MzIxNzExOA%3D%3D; havana_tgc:eyJjcmVhdGVUaW1lIjoxNTgzMjE3MTE4MjA3LCJsYW5nIjoiemhfQ04iLCJwYXRpYWxUZ2MiOnsiYWNjSW5mb3MiOnsiMCI6eyJhY2Nlc3NUeXBlIjoxLCJtZW1iZXJJZCI6MjQ0Mjg0ODUwOSwidGd0SWQiOiIxbEIwMTFVcVlRMVEtc2RNMFQzUUpJdyJ9fX19; lc:Vyu3o7Ozk3Ef%2F56yBA%3D%3D; lgc:%5Cu9E21%5Cu722A%5Cu91CA%5Cu5C0A; lid:%E9%B8%A1%E7%88%AA%E9%87%8A%E5%B0%8A; log:lty=Tmc%3D; sg:%E5%B0%8A9f; sgcookie:DFd4goj9nNVDm8XPOEKL5; skt:ada0d70084368a0a; t:da408da31dc3782c0e142cd1a5ce56d; tg:0; tracknick:%5Cu9E21%5Cu722A%5Cu91CA%5Cu5C0A; uc1:pas=0&amp;cookie21=U%2BGCWk%2F7pY%2FF&amp;tag=8&amp;cookie14=UoTUOaxh3ORVcQ%3D%3D&amp;lng=zh_CN&amp;cookie15=WqG3DMC9VAQiUQ%3D%3D&amp;cookie16=UIHiLt3xCS3yM2h4eKHS9lpEOw%3D%3D&amp;existShop=false; uc3:id2=UUwU26AEyM8whA%3D%3D&amp;lg2=UIHiLt3xD8xYTw%3D%3D&amp;nk2=3z9%2FtN4fGj4%3D&amp;vt3=F8dBxd35YioHqXT24HU%3D; uc4:id4=0%40U27L9u0BzJYiJHlDUhv%2Fp9U77FPP&amp;nk4=0%403cvz%2Bum4meUwidekhs3ybmXXuQ%3D%3D; unb:2442848509; _samesite_flag_:true; cookieCheck:88161; _mw_us_time_:1583217117680; ', 'dnt': '1', 'origin': 'https://item.taobao.com', 'referer': 'https://item.taobao.com/item.htm?spm=a230r.1.14.54.23c23e23NNwLV3&amp;id=608583590222&amp;ns=1&amp;abbucket=8', 'sec-fetch-dest': 'document', 'sec-fetch-mode': 'navigate', 'sec-fetch-site': 'same-site', 'sec-fetch-user': '?1', 'upgrade-insecure-requests': '1', 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.122 Safari/537.36', &#125; # 登录toabao.com提交的数据，如果登录失败，可以从浏览器复制你的form data data = &#123; 'onekey': '', 'gmtCreate': '', 'checkCodeIds': '', 'secStrNoCCode': '', 'tb_token': 'ebe54b3be8765', 'item_url_refer': 'https://s.taobao.com/search?initiative_id=tbindexz_20170306&amp;ie=utf8&amp;spm=a21bo.2017.201856-taobao-item.2&amp;sourceId=tb.index&amp;search_type=item&amp;ssid=s5-e&amp;commend=all&amp;imgfile=&amp;q=%E5%B0%8F%E7%B1%B36%E5%B1%8F%E5%B9%95%E6%80%BB%E6%88%90&amp;suggest=history_1&amp;_input_charset=utf-8&amp;wq=&amp;suggest_query=&amp;source=suggest', 'item_id': '608583590222', 'item_id_num': '608583590222', 'auction_type': 'b', 'from': 'item_detail', 'frm': 'undefined', 'current_price': '85.00', 'auto_post1': '', 'quantity': 1, 'skuId': '4273694133892', 'skuInfo': '', 'buyer_from': 'undefined', 'chargeTypeId': '', 'root_refer': 'undefined', 'source_time': '1583214363061', 'ybhpss': 'undefined', 'fuwubaoInfo': '' &#125; try: response = s.post(url, headers=headers, data=data,timeout=self.timeout) #print(response.text) except Exception as e: print('订单页面返回失败：') raise eif __name__ == '__main__': # 淘宝用户名 username = 'nick_name' # 淘宝重要参数，从浏览器或抓包工具中复制，可重复使用 ua = ua = '122#j42FSDsJEExZUJpZREpjEJponDJE7SNEEP7rEJ+gu4Ep+BzopCG6p+TzFolE7SNpvl7ZpJRcuJPE+BNPpC76EJponDJLqgNEEPGrpJ+gu4Ep+FQPpoGUEELWn4yP7fypE2YuAOzowf+mczd4xQwPZJ/BHr3XHPb612cVDBA3nzdot66aptZudF+Dqm/BJ24kjwUbLhxI0LlIU51t2b0WyUfzIyADqMfpMNnU7qvmuOIEDnGhSorLyNbbyFfmqMf2ENpangL4uO8pELVZGWZRfsTbyFfDmSfbEEpMnMp1uOIEELXZ8oL6JNEERBfmqM32E5pangL4uljEDLVr8CpUJ4bEyNRDqMfjmDpxngLV8Om9E5G+PkoNmzJL4EXwIfwIGRkP+e7gnDdSoSYYYq6rr62jtZ2O23Pn6VlFi6S2wNfeLySDVaS2mshl7w2LnslQuZ/0C1dz8K4pjth2hllY88eBtF8y9QdqXBNrRfLMt9GtnL4cfMJJWJuazlRSEg+GWhtOREosfn8BQjy44CRZ7mJjnk5HcuwUA/KqD+3E40DFHJ6dJobSNkmncMPKvdi4bMf+uTwPm2xP6QS+6LmY8xYD8vqGhm9FVyoYHTv3Hnz9PnE+wWQLUrOJ2nMNMYZRPbXHnqDqv9Nu4uhh6AAyCAMKLw/y2BCxYmw2hRGq6rvY9nff7szqg54eHGAKObJURFRrVqHMDjAAG9Si9Hh0vSJfSBzUc0UjvyvICN+qbiYyTyemP9pcl7juaLgAl3zrHIth0chZF8ejlAva9t3/h7Xay97SmfdYI9tSkyEmKxvz2XnUYaDe5h/S9o4qqqhhRTeRQj6ZvBivU+Uaixj6w4lsT4ykgh3kkk0EJg1vLHMyKzyw0H7vj9fqdHMryMCWdDkqC8TPjJUMe8x70q9/+1qdvnYdHvbwtzT+oh6FgdTB96Y3//GG5wk6jjdn0Ty8uzBZ44mXs+K9D5Ff' # 加密后的密码，从浏览器或抓包工具中复制，可重复使用 TPL_password2 = '906c31076803ea0b0b1e56d2c5f5fe4e7141b054a90bbc52d5abd70b2b16d472c4310a1530a44cec2556be4b14fc0e9755b2d0efd84d7b2e424ea9334353af0d9f215fb51535ea1c83d522604cb904095480294f3eb9d9c211cba06e997f655e952c971828e5725c476e84693fd574910dba44ee6d6429824caba9745b0d802b' ul = UsernameLogin(username, ua, TPL_password2) ul.login() #ul.order()","categories":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/categories/python/"},{"name":"爬虫","slug":"python/爬虫","permalink":"https://blog.paomax.com/categories/python/%E7%88%AC%E8%99%AB/"},{"name":"待研究","slug":"python/爬虫/待研究","permalink":"https://blog.paomax.com/categories/python/%E7%88%AC%E8%99%AB/%E5%BE%85%E7%A0%94%E7%A9%B6/"}],"tags":[{"name":"python","slug":"python","permalink":"https://blog.paomax.com/tags/python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://blog.paomax.com/tags/%E7%88%AC%E8%99%AB/"}]},{"title":"单变量线性回归","slug":"单变量线性回归","date":"2019-07-30T01:08:39.000Z","updated":"2021-02-20T05:08:10.492Z","comments":true,"path":"2019/07/30/0908.html","link":"","permalink":"https://blog.paomax.com/2019/07/30/0908.html","excerpt":"","text":"单变量线性回归 重点知识： 线性回归 代价函数 梯度下降 1.模型描述 建立一个预测房屋价格的模型，如图： 其中直线的直线可以表示为： hθ(x)=θ0+θ1xh_\\theta(x) = \\theta_0 + \\theta_1x hθ​(x)=θ0​+θ1​x 解释：这其实就是小学学习的一次函数 2. 代价函数 为了让直线更好的拟合函数，用代价函数来缩小直线和数据之间的差距。 代价函数形式如下： J(θ0,θ1)=12m∑i=1m(hθ(x(i))−y(i))2J(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum^{m}_{i=1}{(h_\\theta(x^{(i)})}-y^{(i)})^2 J(θ0​,θ1​)=2m1​i=1∑m​(hθ​(x(i))−y(i))2 解释：x(i)x^{(i)}x(i)中的i指的是第i组数据，求这个函数的最小值，得出的θ\\thetaθ就是最拟合数据的直线的值 问题： 为什么是12m\\frac{1}{2m}2m1​而不是1m\\frac{1}{m}m1​呢? 答：我理解是为了求导数时候的平方可以和这个12\\frac{1}{2}21​抵消，简化结果，而且求最小值和最小值的一半是一样的。 3. 梯度下降 梯度下降就是一种找J(θ0,θ1)J(\\theta_0,\\theta_1)J(θ0​,θ1​)最小值的算法，描述如下： repeat until convergence{ θj=θj−αδδθjJ(θ0,θ1)\\theta_j = \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j} J(\\theta_0,\\theta_1)θj​=θj​−αδθj​δ​J(θ0​,θ1​) (for j=0 and j=1) } 解释：α\\alphaα为学习率，代表寻找最小值时候的跨度。就是通过循环寻找θj\\theta_jθj​，直到找到θj\\theta_jθj​。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.paomax.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","permalink":"https://blog.paomax.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"}]},{"title":"编程作业ex2","slug":"编程作业ex2","date":"2019-07-29T07:30:39.000Z","updated":"2021-02-20T05:08:10.509Z","comments":true,"path":"2019/07/29/1530.html","link":"","permalink":"https://blog.paomax.com/2019/07/29/1530.html","excerpt":"","text":"编程练习-逻辑回归 octave实现 1. 可视化数据 通常将数据可视化比较好 ploteData.m 加入如下代码： 123456% Find Indices of Positive and Negative Examplespos = find(y == 1);neg = find(y == 0);% Plot Examplesplot(X(pos,1), X(pos,2), 'k+', 'LineWidth', 2, 'MarkerSize', 7);plot(X(neg,1),X(neg,2), 'ko', 'MarkerFaceColor', 'y', 'MarkerSize', 7); 得到如下图： 2.实现 2.1代价函数 代价函数和梯度下降 costFunction.m 加入如下代码 1234567891011for i=1:m J = J+(y(i)*log(sigmoid(X(i,:)*theta))+(1-y(i))*log(1-sigmoid(X(i,:)*theta)));endJ = J/(-m);% compute Gradient Descentfor j=1:size(grad) for i=1:m grad(j) = grad(j)+(sigmoid(X(i,:)*theta)-y(i))*X(i,j); end grad(j) = grad(j)/m;end 2.1 找合适的θ\\thetaθ值 这里并没有自己写θ\\thetaθ函数，用的octave自带的fminunc函数 代码如下： 123456 % Set options for fminunc options = optimset('GradObj', 'on', 'MaxIter', 400);% Run fminunc to obtain the optimal theta % This function will return theta and the cost [theta, cost] = fminunc(@(t)(costFunction(t, X, y)),initial theta, options); 得到如下代码 解释: fminunc相当于让X和y不变，只改变t也就是θ\\thetaθ，让costFuntion是关于θ\\thetaθ的函数。 问题： 为什么fminunc函数这样用呢？ 答： 可能软件就是这样规定的吧，costFunction按照这种形式（一部分是代价函数数值J，一部分是下降梯度grad），fminunc函数自动调用这两个变量来计算最优值。 python实现","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.paomax.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","permalink":"https://blog.paomax.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"}]},{"title":"编程作业ex1","slug":"编程作业ex1","date":"2019-07-29T05:24:39.000Z","updated":"2021-02-20T05:08:10.508Z","comments":true,"path":"2019/07/29/1324.html","link":"","permalink":"https://blog.paomax.com/2019/07/29/1324.html","excerpt":"","text":"编程练习-线性回归 octave实现 1. 一个简单的热身程序 返回一个5×5的矩阵 1A = eye(5); octave输出如下： 2. 单变量线性回归 注释： 此程序的入口程序是ex1.m 在本练习的这一部分中，将使用一个变量实现线性回归来预测一辆食品卡车的利润。假设一家连锁餐厅的首席执行官，正在考虑在不同的城市开设一家新餐厅。这个连锁店已经在各个城市有了卡车，通过计算可以得到城市的利润和人口数据。 2.1 绘制数据 需要补全的是plotData.m这个文件 程序如下： 123plot(x, y, 'rx', 'MarkerSize', 10); % 绘制数据ylabel('Profit in $10,000s'); % y轴的文字提示 xlabel('Population of City in 10,000s'); % x轴的文字提示 运行得到如下图： 2.2 梯度下降 2.2.1 更新方程 代价函数： J(θ)=12m∑i=1m(hθ(x(i))−y(i))2J(\\theta) = \\frac{1}{2m} \\sum^{m}_{i=1}{(h_\\theta(x^{(i)})}-y^{(i)})^2 J(θ)=2m1​i=1∑m​(hθ​(x(i))−y(i))2 线性方程： hθ(x)=θTx=θ0+θ1x1h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1x_1 hθ​(x)=θTx=θ0​+θ1​x1​ θ\\thetaθ更新方程： θj:=θj−αδδθjJ(θ0,θ1)=θj−α1m∑i=1m(hθ(x(i))−y(i))xj(i)\\theta_j := \\theta_j - \\alpha \\frac{\\delta}{\\delta\\theta_j} J(\\theta_0,\\theta_1) = \\theta_j - \\alpha \\frac{1}{m} \\sum^{m}_{i=1}{(h_\\theta(x^{(i)})}-y^{(i)})x ^{(i)}_j θj​:=θj​−αδθj​δ​J(θ0​,θ1​)=θj​−αm1​i=1∑m​(hθ​(x(i))−y(i))xj(i)​ 2.3 代码实现 对X,theta,迭代次数，学习率做以下初始化： 123X = [ones(m, 1), data(:,1)]; % 增加一列 1 到 x theta = zeros(2, 1); % 初始化theta为 0iterations = 1500; alpha = 0.01; %求最小值的迭代次数为1500 2.3.1 求代价函数j(θ)j(\\theta)j(θ) computeCost.m文件添加以下代码： 1234 for i = 1:m J = J+(X(i,:)*theta-y(i))^2;endJ = J/(2*m); 2.3.2 实现梯度下降 gradientDescent.m文件添加以下代码 12345678sum1 = 0;%初始化为0sum2 = 0;for i = 1:m %循环计算偏导数 sum1 = sum1+(X(i,:)*theta-y(i)); sum2 = sum2+(X(i,:)*theta-y(i))*X(i,2);end %还在一个大循环里面，循环更新theta(1) = theta(1)-alpha/m*sum1;theta(2) = theta(2)-alpha/m*sum2; 得到下图： 2.3.2 可视化 J(θ)J(\\theta)J(θ) 此代码在作业自带源代码中 执行后显示如下两个图： python实现 1. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326#!/usr/bin/env python# coding: utf-8# # 机器学习练习 1 - 线性回归# 这个是另一位大牛写的，作业内容在根目录： [作业文件](ex1.pdf)# # 代码修改并注释：黄海广，haiguang2000@qq.com# ## 单变量线性回归# In[1]:import numpy as npimport pandas as pdimport matplotlib.pyplot as plt# In[2]:path = 'ex1data1.txt'data = pd.read_csv(path, header=None, names=['Population', 'Profit'])data.head()# In[3]:data.describe()# 看下数据长什么样子# In[4]:data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))plt.show()# 现在让我们使用梯度下降来实现线性回归，以最小化成本函数。 以下代码示例中实现的方程在“练习”文件夹中的“ex1.pdf”中有详细说明。# 首先，我们将创建一个以参数θ为特征函数的代价函数#$$ J\\left( \\theta \\right)=\\frac&#123;1&#125;&#123;2m&#125;\\sum\\limits_&#123;i=1&#125;^&#123;m&#125;&#123;&#123;&#123;\\left( &#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( &#123;&#123;x&#125;^&#123;(i)&#125;&#125; \\right)-&#123;&#123;y&#125;^&#123;(i)&#125;&#125; \\right)&#125;^&#123;2&#125;&#125;&#125; $$# 其中：\\\\[&#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( x \\right)=&#123;&#123;\\theta &#125;^&#123;T&#125;&#125;X=&#123;&#123;\\theta &#125;_&#123;0&#125;&#125;&#123;&#123;x&#125;_&#123;0&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;1&#125;&#125;&#123;&#123;x&#125;_&#123;1&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;2&#125;&#125;&#123;&#123;x&#125;_&#123;2&#125;&#125;+...+&#123;&#123;\\theta &#125;_&#123;n&#125;&#125;&#123;&#123;x&#125;_&#123;n&#125;&#125;\\\\]# In[5]:def computeCost(X, y, theta): inner = np.power(((X * theta.T) - y), 2) return np.sum(inner) / (2 * len(X))# 让我们在训练集中添加一列，以便我们可以使用向量化的解决方案来计算代价和梯度。# In[6]:data.insert(0, 'Ones', 1)# 现在我们来做一些变量初始化。# In[7]:# set X (training data) and y (target variable)cols = data.shape[1]X = data.iloc[:,0:cols-1]#X是所有行，去掉最后一列y = data.iloc[:,cols-1:cols]#X是所有行，最后一列# 观察下 X (训练集) and y (目标变量)是否正确.# In[8]:X.head()#head()是观察前5行# In[9]:y.head()# 代价函数是应该是numpy矩阵，所以我们需要转换X和Y，然后才能使用它们。 我们还需要初始化theta。# In[10]:X = np.matrix(X.values)y = np.matrix(y.values)theta = np.matrix(np.array([0,0]))# theta 是一个(1,2)矩阵# In[11]:theta# 看下维度# In[12]:X.shape, theta.shape, y.shape# 计算代价函数 (theta初始值为0).# In[13]:computeCost(X, y, theta)# # batch gradient decent（批量梯度下降）# $$&#123;&#123;\\theta &#125;_&#123;j&#125;&#125;:=&#123;&#123;\\theta &#125;_&#123;j&#125;&#125;-\\alpha \\frac&#123;\\partial &#125;&#123;\\partial &#123;&#123;\\theta &#125;_&#123;j&#125;&#125;&#125;J\\left( \\theta \\right)$$# In[14]:def gradientDescent(X, y, theta, alpha, iters): temp = np.matrix(np.zeros(theta.shape)) parameters = int(theta.ravel().shape[1]) cost = np.zeros(iters) for i in range(iters): error = (X * theta.T) - y for j in range(parameters): term = np.multiply(error, X[:,j]) temp[0,j] = theta[0,j] - ((alpha / len(X)) * np.sum(term)) theta = temp cost[i] = computeCost(X, y, theta) return theta, cost# 初始化一些附加变量 - 学习速率α和要执行的迭代次数。# In[15]:alpha = 0.01iters = 1000# 现在让我们运行梯度下降算法来将我们的参数θ适合于训练集。# In[16]:g, cost = gradientDescent(X, y, theta, alpha, iters)g# 最后，我们可以使用我们拟合的参数计算训练模型的代价函数（误差）。# In[17]:computeCost(X, y, g)# 现在我们来绘制线性模型以及数据，直观地看出它的拟合。# In[18]:x = np.linspace(data.Population.min(), data.Population.max(), 100)f = g[0, 0] + (g[0, 1] * x)fig, ax = plt.subplots(figsize=(12,8))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Traning Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')plt.show()# 由于梯度方程式函数也在每个训练迭代中输出一个代价的向量，所以我们也可以绘制。 请注意，代价总是降低 - 这是凸优化问题的一个例子。# In[19]:fig, ax = plt.subplots(figsize=(12,8))ax.plot(np.arange(iters), cost, 'r')ax.set_xlabel('Iterations')ax.set_ylabel('Cost')ax.set_title('Error vs. Training Epoch')plt.show()# ## 多变量线性回归# 练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。# In[20]:path = 'ex1data2.txt'data2 = pd.read_csv(path, header=None, names=['Size', 'Bedrooms', 'Price'])data2.head()# 对于此任务，我们添加了另一个预处理步骤 - 特征归一化。 这个对于pandas来说很简单# In[21]:data2 = (data2 - data2.mean()) / data2.std()data2.head()# 现在我们重复第1部分的预处理步骤，并对新数据集运行线性回归程序。# In[22]:# add ones columndata2.insert(0, 'Ones', 1)# set X (training data) and y (target variable)cols = data2.shape[1]X2 = data2.iloc[:,0:cols-1]y2 = data2.iloc[:,cols-1:cols]# convert to matrices and initialize thetaX2 = np.matrix(X2.values)y2 = np.matrix(y2.values)theta2 = np.matrix(np.array([0,0,0]))# perform linear regression on the data setg2, cost2 = gradientDescent(X2, y2, theta2, alpha, iters)# get the cost (error) of the modelcomputeCost(X2, y2, g2)# 我们也可以快速查看这一个的训练进程。# In[23]:fig, ax = plt.subplots(figsize=(12,8))ax.plot(np.arange(iters), cost2, 'r')ax.set_xlabel('Iterations')ax.set_ylabel('Cost')ax.set_title('Error vs. Training Epoch')plt.show()# 我们也可以使用scikit-learn的线性回归函数，而不是从头开始实现这些算法。 我们将scikit-learn的线性回归算法应用于第1部分的数据，并看看它的表现。# In[25]:from sklearn import linear_modelmodel = linear_model.LinearRegression()model.fit(X, y)# scikit-learn model的预测表现# In[26]:x = np.array(X[:, 1].A1)f = model.predict(X).flatten()fig, ax = plt.subplots(figsize=(12,8))ax.plot(x, f, 'r', label='Prediction')ax.scatter(data.Population, data.Profit, label='Traning Data')ax.legend(loc=2)ax.set_xlabel('Population')ax.set_ylabel('Profit')ax.set_title('Predicted Profit vs. Population Size')plt.show()# # 4. normal equation（正规方程）# 正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac&#123;\\partial &#125;&#123;\\partial &#123;&#123;\\theta &#125;_&#123;j&#125;&#125;&#125;J\\left( &#123;&#123;\\theta &#125;_&#123;j&#125;&#125; \\right)=0$ 。# 假设我们的训练集特征矩阵为 X（包含了$&#123;&#123;x&#125;_&#123;0&#125;&#125;=1$）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\\theta =&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;&#123;&#123;X&#125;^&#123;T&#125;&#125;y$ 。# 上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A=&#123;&#123;X&#125;^&#123;T&#125;&#125;X$，则：$&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;=&#123;&#123;A&#125;^&#123;-1&#125;&#125;$## 梯度下降与正规方程的比较：## 梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型## 正规方程：不需要选择学习率α，一次计算得出，需要计算$&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n3)$，通常来说当$n$小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型# In[27]:# 正规方程def normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X) return theta# In[28]:final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距final_theta2# In[29]:#梯度下降得到的结果是matrix([[-3.24140214, 1.1272942 ]])# 在练习2中，我们将看看分类问题的逻辑回归。# In[ ]: 2. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442#!/usr/bin/env python# coding: utf-8# # linear regreesion（线性回归）# 注意：python版本为3.6，# 安装TensorFlow的方法：pip install tensorflow# In[1]:import pandas as pdimport seaborn as snssns.set(context=\"notebook\", style=\"whitegrid\", palette=\"dark\")import matplotlib.pyplot as pltimport tensorflow as tfimport numpy as np# In[2]:df = pd.read_csv('ex1data1.txt', names=['population', 'profit'])#读取数据并赋予列名# In[3]:df.head()#看前五行# In[4]:df.info()# ***# # 看下原始数据# In[5]:sns.lmplot('population', 'profit', df, size=6, fit_reg=False)plt.show()# In[6]:def get_X(df):#读取特征# \"\"\"# use concat to add intersect feature to avoid side effect# not efficient for big dataset though# \"\"\" ones = pd.DataFrame(&#123;'ones': np.ones(len(df))&#125;)#ones是m行1列的dataframe data = pd.concat([ones, df], axis=1) # 合并数据，根据列合并 return data.iloc[:, :-1].as_matrix() # 这个操作返回 ndarray,不是矩阵def get_y(df):#读取标签# '''assume the last column is the target''' return np.array(df.iloc[:, -1])#df.iloc[:, -1]是指df的最后一列def normalize_feature(df):# \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\" return df.apply(lambda column: (column - column.mean()) / column.std())#特征缩放# 多变量的假设 h 表示为：\\\\[&#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( x \\right)=&#123;&#123;\\theta &#125;_&#123;0&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;1&#125;&#125;&#123;&#123;x&#125;_&#123;1&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;2&#125;&#125;&#123;&#123;x&#125;_&#123;2&#125;&#125;+...+&#123;&#123;\\theta &#125;_&#123;n&#125;&#125;&#123;&#123;x&#125;_&#123;n&#125;&#125;\\\\] # 这个公式中有n+1个参数和n个变量，为了使得公式能够简化一些，引入$&#123;&#123;x&#125;_&#123;0&#125;&#125;=1$，则公式转化为： # 此时模型中的参数是一个n+1维的向量，任何一个训练实例也都是n+1维的向量，特征矩阵X的维度是 m*(n+1)。 因此公式可以简化为：$&#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( x \\right)=&#123;&#123;\\theta &#125;^&#123;T&#125;&#125;X$，其中上标T代表矩阵转置。# # In[7]:def linear_regression(X_data, y_data, alpha, epoch, optimizer=tf.train.GradientDescentOptimizer):# 这个函数是旧金山的一个大神Lucas Shen写的 # placeholder for graph input X = tf.placeholder(tf.float32, shape=X_data.shape) y = tf.placeholder(tf.float32, shape=y_data.shape) # construct the graph with tf.variable_scope('linear-regression'): W = tf.get_variable(\"weights\", (X_data.shape[1], 1), initializer=tf.constant_initializer()) # n*1 y_pred = tf.matmul(X, W) # m*n @ n*1 -&gt; m*1 loss = 1 / (2 * len(X_data)) * tf.matmul((y_pred - y), (y_pred - y), transpose_a=True) # (m*1).T @ m*1 = 1*1 opt = optimizer(learning_rate=alpha) opt_operation = opt.minimize(loss) # run the session with tf.Session() as sess: sess.run(tf.global_variables_initializer()) loss_data = [] for i in range(epoch): _, loss_val, W_val = sess.run([opt_operation, loss, W], feed_dict=&#123;X: X_data, y: y_data&#125;) loss_data.append(loss_val[0, 0]) # because every loss_val is 1*1 ndarray if len(loss_data) &gt; 1 and np.abs(loss_data[-1] - loss_data[-2]) &lt; 10 ** -9: # early break when it's converged # print('Converged at epoch &#123;&#125;'.format(i)) break # clear the graph tf.reset_default_graph() return &#123;'loss': loss_data, 'parameters': W_val&#125; # just want to return in row vector format# In[8]:data = pd.read_csv('ex1data1.txt', names=['population', 'profit'])#读取数据，并赋予列名data.head()#看下数据前5行# # 计算代价函数# $$J\\left( \\theta \\right)=\\frac&#123;1&#125;&#123;2m&#125;\\sum\\limits_&#123;i=1&#125;^&#123;m&#125;&#123;&#123;&#123;\\left( &#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( &#123;&#123;x&#125;^&#123;(i)&#125;&#125; \\right)-&#123;&#123;y&#125;^&#123;(i)&#125;&#125; \\right)&#125;^&#123;2&#125;&#125;&#125;$$# 其中：\\\\[&#123;&#123;h&#125;_&#123;\\theta &#125;&#125;\\left( x \\right)=&#123;&#123;\\theta &#125;^&#123;T&#125;&#125;X=&#123;&#123;\\theta &#125;_&#123;0&#125;&#125;&#123;&#123;x&#125;_&#123;0&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;1&#125;&#125;&#123;&#123;x&#125;_&#123;1&#125;&#125;+&#123;&#123;\\theta &#125;_&#123;2&#125;&#125;&#123;&#123;x&#125;_&#123;2&#125;&#125;+...+&#123;&#123;\\theta &#125;_&#123;n&#125;&#125;&#123;&#123;x&#125;_&#123;n&#125;&#125;\\\\] # In[9]:X = get_X(data)print(X.shape, type(X))y = get_y(data)print(y.shape, type(y))#看下数据维度# In[10]:theta = np.zeros(X.shape[1])#X.shape[1]=2,代表特征数n# In[11]:def lr_cost(theta, X, y):# \"\"\"# X: R(m*n), m 样本数, n 特征数# y: R(m)# theta : R(n), 线性回归的参数# \"\"\" m = X.shape[0]#m为样本数 inner = X @ theta - y # R(m*1)，X @ theta等价于X.dot(theta) # 1*m @ m*1 = 1*1 in matrix multiplication # but you know numpy didn't do transpose in 1d array, so here is just a # vector inner product to itselves square_sum = inner.T @ inner cost = square_sum / (2 * m) return cost# In[12]:lr_cost(theta, X, y)#返回theta的值# # batch gradient decent（批量梯度下降）# $$&#123;&#123;\\theta &#125;_&#123;j&#125;&#125;:=&#123;&#123;\\theta &#125;_&#123;j&#125;&#125;-\\alpha \\frac&#123;\\partial &#125;&#123;\\partial &#123;&#123;\\theta &#125;_&#123;j&#125;&#125;&#125;J\\left( \\theta \\right)$$# In[13]:def gradient(theta, X, y): m = X.shape[0] inner = X.T @ (X @ theta - y) # (m,n).T @ (m, 1) -&gt; (n, 1)，X @ theta等价于X.dot(theta) return inner / m# In[14]:def batch_gradient_decent(theta, X, y, epoch, alpha=0.01):# 拟合线性回归，返回参数和代价# epoch: 批处理的轮数# \"\"\" cost_data = [lr_cost(theta, X, y)] _theta = theta.copy() # 拷贝一份，不和原来的theta混淆 for _ in range(epoch): _theta = _theta - alpha * gradient(_theta, X, y) cost_data.append(lr_cost(_theta, X, y)) return _theta, cost_data#批量梯度下降函数# In[15]:epoch = 500final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch)# In[16]:final_theta#最终的theta# In[17]:cost_data# 看下代价数据# In[18]:# 计算最终的代价lr_cost(final_theta, X, y)# # visualize cost data（代价数据可视化）# In[19]:ax = sns.tsplot(cost_data, time=np.arange(epoch+1))ax.set_xlabel('epoch')ax.set_ylabel('cost')plt.show()#可以看到从第二轮代价数据变换很大，接下来平稳了# In[20]:b = final_theta[0] # intercept，Y轴上的截距m = final_theta[1] # slope，斜率plt.scatter(data.population, data.profit, label=\"Training data\")plt.plot(data.population, data.population*m + b, label=\"Prediction\")plt.legend(loc=2)plt.show()# # 3- 选修章节# In[21]:raw_data = pd.read_csv('ex1data2.txt', names=['square', 'bedrooms', 'price'])raw_data.head()# # 标准化数据# 最简单的方法是令：# # # # 其中 是平均值，sn 是标准差。# # In[22]:def normalize_feature(df):# \"\"\"Applies function along input axis(default 0) of DataFrame.\"\"\" return df.apply(lambda column: (column - column.mean()) / column.std())# In[23]:data = normalize_feature(raw_data)data.head()# # 2. multi-var batch gradient decent（多变量批量梯度下降）# In[24]:X = get_X(data)print(X.shape, type(X))y = get_y(data)print(y.shape, type(y))#看下数据的维度和类型# In[25]:alpha = 0.01#学习率theta = np.zeros(X.shape[1])#X.shape[1]：特征数nepoch = 500#轮数# In[26]:final_theta, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha)# In[27]:sns.tsplot(time=np.arange(len(cost_data)), data = cost_data)plt.xlabel('epoch', fontsize=18)plt.ylabel('cost', fontsize=18)plt.show()# In[28]:final_theta# # 3. learning rate（学习率）# In[29]:base = np.logspace(-1, -5, num=4)candidate = np.sort(np.concatenate((base, base*3)))print(candidate)# In[30]:epoch=50fig, ax = plt.subplots(figsize=(16, 9))for alpha in candidate: _, cost_data = batch_gradient_decent(theta, X, y, epoch, alpha=alpha) ax.plot(np.arange(epoch+1), cost_data, label=alpha)ax.set_xlabel('epoch', fontsize=18)ax.set_ylabel('cost', fontsize=18)ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)ax.set_title('learning rate', fontsize=18)plt.show()# # 4. normal equation（正规方程）# 正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：$\\frac&#123;\\partial &#125;&#123;\\partial &#123;&#123;\\theta &#125;_&#123;j&#125;&#125;&#125;J\\left( &#123;&#123;\\theta &#125;_&#123;j&#125;&#125; \\right)=0$ 。# 假设我们的训练集特征矩阵为 X（包含了$&#123;&#123;x&#125;_&#123;0&#125;&#125;=1$）并且我们的训练集结果为向量 y，则利用正规方程解出向量 $\\theta =&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;&#123;&#123;X&#125;^&#123;T&#125;&#125;y$ 。# 上标T代表矩阵转置，上标-1 代表矩阵的逆。设矩阵$A=&#123;&#123;X&#125;^&#123;T&#125;&#125;X$，则：$&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;=&#123;&#123;A&#125;^&#123;-1&#125;&#125;$# # 梯度下降与正规方程的比较：# # 梯度下降：需要选择学习率α，需要多次迭代，当特征数量n大时也能较好适用，适用于各种类型的模型 # # 正规方程：不需要选择学习率α，一次计算得出，需要计算$&#123;&#123;\\left( &#123;&#123;X&#125;^&#123;T&#125;&#125;X \\right)&#125;^&#123;-1&#125;&#125;$，如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为O(n3)，通常来说当n小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型# # # In[31]:# 正规方程def normalEqn(X, y): theta = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X) return theta# In[32]:final_theta2=normalEqn(X, y)#感觉和批量梯度下降的theta的值有点差距final_theta2# # run the tensorflow graph over several optimizer# In[33]:X_data = get_X(data)print(X_data.shape, type(X_data))y_data = get_y(data).reshape(len(X_data), 1) # special treatment for tensorflow input dataprint(y_data.shape, type(y_data))# In[34]:epoch = 2000alpha = 0.01# In[35]:optimizer_dict=&#123;'GD': tf.train.GradientDescentOptimizer, 'Adagrad': tf.train.AdagradOptimizer, 'Adam': tf.train.AdamOptimizer, 'Ftrl': tf.train.FtrlOptimizer, 'RMS': tf.train.RMSPropOptimizer &#125;results = []for name in optimizer_dict: res = linear_regression(X_data, y_data, alpha, epoch, optimizer=optimizer_dict[name]) res['name'] = name results.append(res)# # 画图# In[36]:fig, ax = plt.subplots(figsize=(16, 9))for res in results: loss_data = res['loss'] # print('for optimizer &#123;&#125;'.format(res['name']))# print('final parameters\\n', res['parameters'])# print('final loss=&#123;&#125;\\n'.format(loss_data[-1])) ax.plot(np.arange(len(loss_data)), loss_data, label=res['name'])ax.set_xlabel('epoch', fontsize=18)ax.set_ylabel('cost', fontsize=18)ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)ax.set_title('different optimizer', fontsize=18)plt.show()# In[ ]:","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.paomax.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","permalink":"https://blog.paomax.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"}]},{"title":"绪论：初识机器学习","slug":"绪论：初识机器学习","date":"2019-07-16T02:05:40.000Z","updated":"2021-02-20T05:08:10.493Z","comments":true,"path":"2019/07/16/1005.html","link":"","permalink":"https://blog.paomax.com/2019/07/16/1005.html","excerpt":"","text":"绪论：初识机器学习 重点知识： 什么是机器学习 监督学习 无监督学习 备注：本节知识为概念类知识，一些定义搜集于搜索引擎 什么是机器学习 机器学习(Machine Learning, ML)是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。 监督学习 监督学习是指：利用一组已知类别的样本调整分类器的参数，使其达到所要求性能的过程，也称为监督训练或有教师学习。 监督学习是从标记的训练数据来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。监督学习算法是分析该训练数据，并产生一个推断的功能，其可以用于映射出新的实例。一个最佳的方案将允许该算法来正确地决定那些看不见的实例的类标签。这就要求学习算法是在一种“合理”的方式从一种从训练数据到看不见的情况下形成。 无监督学习 现实生活中常常会有这样的问题：缺乏足够的先验知识，因此难以人工标注类别或进行人工类别标注的成本太高。很自然地，我们希望计算机能代我们完成这些工作，或至少提供一些帮助。根据类别未知(没有被标记)的训练样本解决模式识别中的各种问题，称之为无监督学习。","categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"人工智能","slug":"人工智能","permalink":"https://blog.paomax.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"机器学习","slug":"机器学习","permalink":"https://blog.paomax.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"吴恩达","slug":"吴恩达","permalink":"https://blog.paomax.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"},{"name":"绪论","slug":"绪论","permalink":"https://blog.paomax.com/tags/%E7%BB%AA%E8%AE%BA/"}]}]}